{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pair-wise transfer learning\n",
        "\n",
        "This exercise demonstrates a transfer learning approach using the ALIGNN architecture. We will first pre-train an ALIGNN model on a dataset of formation energies. Subsequently, we will employ this pre-trained model as the initialization point for training a second ALIGNN model tasked with predicting the highest optical phonon mode frequency. This fine-tuning strategy will involve updating all model parameters during the training phase for the phonon mode frequency prediction task. We will then compare the predictive performance of this fine-tuned model against that of a baseline ALIGNN model, which we shall refer to as the \"SCRATCH\" model, trained exclusively on the phonon mode frequency data. This comparison will highlight the potential benefits of leveraging pre-training for downstream property prediction. This specific fine-tuning methodology, where all layers are retrained, is one of several possible strategies detailed in the relevant literature on transfer learning and is chosen here for illustrative purposes."
      ],
      "metadata": {
        "id": "xEcr9NH473sW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "Install and initialize condacolab, a library that allows you to use the Conda package manager within a Google Colab environment\n"
      ],
      "metadata": {
        "id": "C1xYRvrh8sO4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6COCVWiIpxw",
        "outputId": "71e277d2-dd42-498d-fad7-829bf0ce1690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è¨ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:26\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64DN2NhyIvhR",
        "outputId": "d4b17d42-b3ec-467c-b380-8d88b022f623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "# import condacolab\n",
        "# condacolab.check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwI6cP55-L3z"
      },
      "source": [
        "Install ALIGNN  (It might take ~ 6-8 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGOyZWtEIv1E",
        "outputId": "3d5fc28e-608c-43af-df73-a690707c3e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channels:\n",
            " - pytorch\n",
            " - nvidia\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - alignn\n",
            "    - dgl=2.1.0\n",
            "    - pytorch\n",
            "    - pytorch-cuda\n",
            "    - torchaudio\n",
            "    - torchvision\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       2_kmp_llvm           6 KB  conda-forge\n",
            "    absl-py-2.1.0              |     pyhd8ed1ab_1         105 KB  conda-forge\n",
            "    alignn-2024.10.30          |     pyhd8ed1ab_0          76 KB  conda-forge\n",
            "    annotated-types-0.7.0      |     pyhd8ed1ab_1          18 KB  conda-forge\n",
            "    ase-3.23.0                 |     pyhd8ed1ab_0         1.8 MB  conda-forge\n",
            "    astunparse-1.6.3           |     pyhd8ed1ab_2          18 KB  conda-forge\n",
            "    babel-2.16.0               |     pyhd8ed1ab_1         6.2 MB  conda-forge\n",
            "    blinker-1.9.0              |     pyhff2d567_0          14 KB  conda-forge\n",
            "    brotli-1.1.0               |       hd590300_1          19 KB  conda-forge\n",
            "    brotli-bin-1.1.0           |       hd590300_1          19 KB  conda-forge\n",
            "    c-ares-1.34.4              |       hb9d3cd8_0         201 KB  conda-forge\n",
            "    ca-certificates-2024.12.14 |       hbcca054_0         153 KB  conda-forge\n",
            "    cached-property-1.5.2      |       hd8ed1ab_1           4 KB  conda-forge\n",
            "    cached_property-1.5.2      |     pyha770c72_1          11 KB  conda-forge\n",
            "    certifi-2024.12.14         |     pyhd8ed1ab_0         158 KB  conda-forge\n",
            "    click-8.1.8                |     pyh707e725_0          83 KB  conda-forge\n",
            "    contourpy-1.3.1            |  py310h3788b33_0         255 KB  conda-forge\n",
            "    cuda-cudart-12.4.127       |                0         198 KB  nvidia\n",
            "    cuda-cupti-12.4.127        |                0        16.4 MB  nvidia\n",
            "    cuda-libraries-12.4.1      |                0           2 KB  nvidia\n",
            "    cuda-nvrtc-12.4.127        |                0        21.0 MB  nvidia\n",
            "    cuda-nvtx-12.4.127         |                0          58 KB  nvidia\n",
            "    cuda-opencl-12.6.77        |                0          25 KB  nvidia\n",
            "    cuda-runtime-12.4.1        |                0           2 KB  nvidia\n",
            "    cuda-version-12.6          |                3          16 KB  nvidia\n",
            "    cycler-0.12.1              |     pyhd8ed1ab_1          13 KB  conda-forge\n",
            "    dgl-2.1.0                  |  py310h039f8d5_2         4.0 MB  conda-forge\n",
            "    ffmpeg-4.3                 |       hf484d3e_0         9.9 MB  pytorch\n",
            "    filelock-3.16.1            |     pyhd8ed1ab_1          17 KB  conda-forge\n",
            "    flake8-7.1.1               |     pyhd8ed1ab_1         109 KB  conda-forge\n",
            "    flask-3.1.0                |     pyhff2d567_0          80 KB  conda-forge\n",
            "    flatbuffers-24.3.25        |       h59595ed_0         1.4 MB  conda-forge\n",
            "    fonttools-4.55.3           |  py310h89163eb_1         2.2 MB  conda-forge\n",
            "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
            "    fsspec-2024.12.0           |     pyhd8ed1ab_0         135 KB  conda-forge\n",
            "    gast-0.6.0                 |     pyhd8ed1ab_0          24 KB  conda-forge\n",
            "    ghp-import-2.1.0           |     pyhd8ed1ab_2          16 KB  conda-forge\n",
            "    giflib-5.2.2               |       hd590300_0          75 KB  conda-forge\n",
            "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
            "    gmpy2-2.1.5                |  py310he8512ff_3         198 KB  conda-forge\n",
            "    gnutls-3.6.13              |       h85f3911_1         2.0 MB  conda-forge\n",
            "    google-pasta-0.2.0         |     pyhd8ed1ab_2          48 KB  conda-forge\n",
            "    grpcio-1.62.2              |  py310h1b8f574_0         980 KB  conda-forge\n",
            "    h5py-3.12.1                |nompi_py310h60e0fe6_103         1.2 MB  conda-forge\n",
            "    hdf5-1.14.3                |nompi_hdf9ad27_105         3.7 MB  conda-forge\n",
            "    importlib-metadata-8.5.0   |     pyha770c72_1          28 KB  conda-forge\n",
            "    importlib-resources-6.4.5  |     pyhd8ed1ab_1           9 KB  conda-forge\n",
            "    importlib_resources-6.4.5  |     pyhd8ed1ab_1          32 KB  conda-forge\n",
            "    inflect-7.5.0              |     pyhd8ed1ab_0          37 KB  conda-forge\n",
            "    itsdangerous-2.2.0         |     pyhd8ed1ab_1          19 KB  conda-forge\n",
            "    jarvis-tools-2024.10.30    |     pyhd8ed1ab_0         3.8 MB  conda-forge\n",
            "    jinja2-3.1.5               |     pyhd8ed1ab_0         110 KB  conda-forge\n",
            "    joblib-1.4.2               |     pyhd8ed1ab_1         215 KB  conda-forge\n",
            "    keras-3.7.0                |     pyh753f3f9_1         681 KB  conda-forge\n",
            "    kiwisolver-1.4.7           |  py310h3788b33_0          70 KB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    lcms2-2.16                 |       hb7c19ff_0         239 KB  conda-forge\n",
            "    lerc-4.0.0                 |       h27087fc_0         275 KB  conda-forge\n",
            "    libabseil-20240116.2       | cxx17_he02047a_1         1.2 MB  conda-forge\n",
            "    libaec-1.1.3               |       h59595ed_0          35 KB  conda-forge\n",
            "    libblas-3.9.0              |26_linux64_openblas          16 KB  conda-forge\n",
            "    libbrotlicommon-1.1.0      |       hd590300_1          68 KB  conda-forge\n",
            "    libbrotlidec-1.1.0         |       hd590300_1          32 KB  conda-forge\n",
            "    libbrotlienc-1.1.0         |       hd590300_1         276 KB  conda-forge\n",
            "    libcblas-3.9.0             |26_linux64_openblas          16 KB  conda-forge\n",
            "    libcublas-12.4.5.8         |                0       309.2 MB  nvidia\n",
            "    libcufft-11.2.1.3          |                0       190.5 MB  nvidia\n",
            "    libcufile-1.11.1.6         |                0         899 KB  nvidia\n",
            "    libcurand-10.3.7.77        |                0        39.7 MB  nvidia\n",
            "    libcurl-8.8.0              |       hca28451_0         396 KB  conda-forge\n",
            "    libcusolver-11.6.1.9       |                0       114.0 MB  nvidia\n",
            "    libcusparse-12.3.1.170     |                0       179.6 MB  nvidia\n",
            "    libdeflate-1.20            |       hd590300_0          70 KB  conda-forge\n",
            "    libgcc-14.2.0              |       h77fa898_1         829 KB  conda-forge\n",
            "    libgcc-ng-14.2.0           |       h69a702a_1          53 KB  conda-forge\n",
            "    libgfortran-14.2.0         |       h69a702a_1          53 KB  conda-forge\n",
            "    libgfortran-ng-14.2.0      |       h69a702a_1          53 KB  conda-forge\n",
            "    libgfortran5-14.2.0        |       hd5240d6_1         1.4 MB  conda-forge\n",
            "    libgomp-14.2.0             |       h77fa898_1         450 KB  conda-forge\n",
            "    libgrpc-1.62.2             |       h15f2491_0         7.0 MB  conda-forge\n",
            "    libhwloc-2.9.3             |default_h554bfaf_1009         2.5 MB  conda-forge\n",
            "    libjpeg-turbo-3.0.0        |       hd590300_1         604 KB  conda-forge\n",
            "    liblapack-3.9.0            |26_linux64_openblas          16 KB  conda-forge\n",
            "    liblapacke-3.9.0           |26_linux64_openblas          16 KB  conda-forge\n",
            "    libnpp-12.2.5.30           |                0       142.8 MB  nvidia\n",
            "    libnvfatbin-12.6.77        |                0         783 KB  nvidia\n",
            "    libnvjitlink-12.4.127      |                0        18.2 MB  nvidia\n",
            "    libnvjpeg-12.3.1.117       |                0         3.0 MB  nvidia\n",
            "    libopenblas-0.3.28         |pthreads_h94d23a6_1         5.3 MB  conda-forge\n",
            "    libpng-1.6.43              |       h2797004_0         281 KB  conda-forge\n",
            "    libprotobuf-4.25.3         |       h08a7969_0         2.7 MB  conda-forge\n",
            "    libre2-11-2023.09.01       |       h5a48ba9_2         227 KB  conda-forge\n",
            "    libsqlite-3.46.0           |       hde9e2c9_0         845 KB  conda-forge\n",
            "    libstdcxx-14.2.0           |       hc0a3c3a_1         3.7 MB  conda-forge\n",
            "    libtiff-4.6.0              |       h1dd3fc0_3         276 KB  conda-forge\n",
            "    libtorch-2.3.1             |cpu_mkl_h0bb0d08_100        47.5 MB  conda-forge\n",
            "    liburing-2.6               |       h297d8ca_0         105 KB  conda-forge\n",
            "    libuv-1.49.2               |       hb9d3cd8_0         864 KB  conda-forge\n",
            "    libwebp-base-1.5.0         |       h851e524_0         420 KB  conda-forge\n",
            "    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n",
            "    llvm-openmp-19.1.6         |       h024ca30_0         3.1 MB  conda-forge\n",
            "    markdown-3.6               |     pyhd8ed1ab_0          76 KB  conda-forge\n",
            "    markdown-it-py-3.0.0       |     pyhd8ed1ab_1          63 KB  conda-forge\n",
            "    markupsafe-3.0.2           |  py310h89163eb_1          23 KB  conda-forge\n",
            "    matplotlib-base-3.10.0     |  py310h68603db_0         7.1 MB  conda-forge\n",
            "    mccabe-0.7.0               |     pyhd8ed1ab_1          13 KB  conda-forge\n",
            "    mdurl-0.1.2                |     pyhd8ed1ab_1          14 KB  conda-forge\n",
            "    mergedeep-1.3.4            |     pyhd8ed1ab_1          11 KB  conda-forge\n",
            "    metis-5.1.1                |       h59595ed_2         3.7 MB  conda-forge\n",
            "    mkdocs-1.6.1               |     pyhd8ed1ab_1         3.4 MB  conda-forge\n",
            "    mkdocs-get-deps-0.2.0      |     pyhd8ed1ab_1          14 KB  conda-forge\n",
            "    mkdocs-material-9.5.49     |     pyhd8ed1ab_0         4.7 MB  conda-forge\n",
            "    mkdocs-material-extensions-1.3.1|     pyhd8ed1ab_1          16 KB  conda-forge\n",
            "    mkl-2023.2.0               |   h84fe81f_50496       156.8 MB  conda-forge\n",
            "    ml_dtypes-0.3.2            |  py310hcc13569_0         160 KB  conda-forge\n",
            "    more-itertools-10.5.0      |     pyhd8ed1ab_1          56 KB  conda-forge\n",
            "    mpc-1.3.1                  |       h24ddda3_1         114 KB  conda-forge\n",
            "    mpfr-4.2.1                 |       h90cbb55_3         620 KB  conda-forge\n",
            "    mpmath-1.3.0               |     pyhd8ed1ab_1         429 KB  conda-forge\n",
            "    munkres-1.1.4              |     pyh9f0ad1d_0          12 KB  conda-forge\n",
            "    namex-0.0.8                |     pyhd8ed1ab_1          11 KB  conda-forge\n",
            "    nettle-3.6                 |       he412f7d_0         6.5 MB  conda-forge\n",
            "    networkx-3.4.2             |     pyh267e887_2         1.2 MB  conda-forge\n",
            "    numpy-1.26.4               |  py310hb13e2d6_0         6.7 MB  conda-forge\n",
            "    openh264-2.1.1             |       h780b84a_0         1.5 MB  conda-forge\n",
            "    openjpeg-2.5.2             |       h488ebb8_0         334 KB  conda-forge\n",
            "    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n",
            "    opt_einsum-3.4.0           |     pyhd8ed1ab_1          61 KB  conda-forge\n",
            "    optree-0.13.1              |  py310h3788b33_1         328 KB  conda-forge\n",
            "    paginate-0.5.7             |     pyhd8ed1ab_1          18 KB  conda-forge\n",
            "    pandas-2.2.3               |  py310h5eaa309_1        12.4 MB  conda-forge\n",
            "    pathspec-0.12.1            |     pyhd8ed1ab_1          40 KB  conda-forge\n",
            "    pillow-10.3.0              |  py310hf73ecf8_0        39.8 MB  conda-forge\n",
            "    protobuf-4.25.3            |  py310h0e2eeba_1         324 KB  conda-forge\n",
            "    psutil-6.1.1               |  py310ha75aee5_0         360 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    hb9d3cd8_1002           8 KB  conda-forge\n",
            "    pycodestyle-2.12.1         |     pyhd8ed1ab_1          34 KB  conda-forge\n",
            "    pydantic-2.10.4            |     pyh3cfb1c2_0         290 KB  conda-forge\n",
            "    pydantic-core-2.27.2       |  py310h505e2c1_0         1.6 MB  conda-forge\n",
            "    pydantic-settings-2.7.1    |     pyh3cfb1c2_0          31 KB  conda-forge\n",
            "    pydocstyle-6.3.0           |     pyhd8ed1ab_1          39 KB  conda-forge\n",
            "    pyflakes-3.2.0             |     pyhd8ed1ab_1          57 KB  conda-forge\n",
            "    pygments-2.18.0            |     pyhd8ed1ab_1         856 KB  conda-forge\n",
            "    pymdown-extensions-10.13   |     pyhd8ed1ab_0         163 KB  conda-forge\n",
            "    pyparsing-2.4.7            |     pyhd8ed1ab_1          60 KB  conda-forge\n",
            "    python-dateutil-2.9.0.post0|     pyhff2d567_1         217 KB  conda-forge\n",
            "    python-dotenv-1.0.1        |     pyhd8ed1ab_1          24 KB  conda-forge\n",
            "    python-flatbuffers-24.3.25 |     pyhe33e51e_1          34 KB  conda-forge\n",
            "    python-lmdb-1.5.1          |  py310h74df5ed_1         130 KB  conda-forge\n",
            "    python-tzdata-2024.2       |     pyhd8ed1ab_1         139 KB  conda-forge\n",
            "    pytorch-2.3.1              |cpu_mkl_py310h75865b9_100        28.9 MB  conda-forge\n",
            "    pytorch-cuda-12.4          |       hc786d27_7           7 KB  pytorch\n",
            "    pytorch-mutex-1.0          |              cpu           3 KB  pytorch\n",
            "    pytz-2024.1                |     pyhd8ed1ab_0         184 KB  conda-forge\n",
            "    pyyaml-6.0.2               |  py310ha75aee5_1         178 KB  conda-forge\n",
            "    pyyaml-env-tag-0.1         |     pyhd8ed1ab_1          10 KB  conda-forge\n",
            "    qhull-2020.2               |       h434a139_5         540 KB  conda-forge\n",
            "    re2-2023.09.01             |       h7f4b329_2          26 KB  conda-forge\n",
            "    regex-2024.11.6            |  py310ha75aee5_0         345 KB  conda-forge\n",
            "    rich-13.9.4                |     pyhd8ed1ab_1         181 KB  conda-forge\n",
            "    scikit-learn-1.6.0         |  py310h27f47ee_0         9.0 MB  conda-forge\n",
            "    scipy-1.14.1               |  py310hfcf56fc_2        16.0 MB  conda-forge\n",
            "    six-1.17.0                 |     pyhd8ed1ab_0          16 KB  conda-forge\n",
            "    sleef-3.7                  |       h1b44611_2         1.8 MB  conda-forge\n",
            "    snappy-1.2.1               |       h8bd8927_1          42 KB  conda-forge\n",
            "    snowballstemmer-2.2.0      |     pyhd8ed1ab_0          57 KB  conda-forge\n",
            "    spglib-2.5.0               |  py310h1e9006d_3         636 KB  conda-forge\n",
            "    sympy-1.13.3               | pypyh2585a3b_103         4.4 MB  conda-forge\n",
            "    tbb-2021.11.0              |       h00ab1b0_1         191 KB  conda-forge\n",
            "    tensorboard-2.16.2         |     pyhd8ed1ab_0         4.9 MB  conda-forge\n",
            "    tensorboard-data-server-0.7.0|  py310h6c63255_2         3.4 MB  conda-forge\n",
            "    tensorflow-2.16.1          |cpu_py310h49b650b_0          41 KB  conda-forge\n",
            "    tensorflow-base-2.16.1     |cpu_py310h224022f_0       147.3 MB  conda-forge\n",
            "    tensorflow-estimator-2.16.1|cpu_py310hc6dcfef_0         540 KB  conda-forge\n",
            "    termcolor-2.5.0            |     pyhd8ed1ab_1          12 KB  conda-forge\n",
            "    threadpoolctl-3.5.0        |     pyhc1e730c_0          23 KB  conda-forge\n",
            "    tomli-2.2.1                |     pyhd8ed1ab_1          19 KB  conda-forge\n",
            "    toolz-1.0.0                |     pyhd8ed1ab_1          51 KB  conda-forge\n",
            "    torchaudio-2.3.1           |        py310_cpu         5.0 MB  pytorch\n",
            "    torchdata-0.9.0            |            py310         2.4 MB  pytorch\n",
            "    torchvision-0.18.1         |        py310_cpu         6.8 MB  pytorch\n",
            "    typeguard-4.4.1            |     pyhd8ed1ab_1          34 KB  conda-forge\n",
            "    typing-extensions-4.12.2   |       hd8ed1ab_1          10 KB  conda-forge\n",
            "    typing_extensions-4.12.2   |     pyha770c72_1          39 KB  conda-forge\n",
            "    unicodedata2-15.1.0        |  py310ha75aee5_1         359 KB  conda-forge\n",
            "    watchdog-6.0.0             |  py310hff52083_0         113 KB  conda-forge\n",
            "    werkzeug-3.1.3             |     pyhd8ed1ab_1         238 KB  conda-forge\n",
            "    wrapt-1.17.0               |  py310ha75aee5_0          55 KB  conda-forge\n",
            "    xmltodict-0.14.2           |     pyhd8ed1ab_1          15 KB  conda-forge\n",
            "    xorg-libxau-1.0.12         |       hb9d3cd8_0          14 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.5        |       hb9d3cd8_0          19 KB  conda-forge\n",
            "    yaml-0.2.5                 |       h7f98852_2          87 KB  conda-forge\n",
            "    zipp-3.21.0                |     pyhd8ed1ab_1          21 KB  conda-forge\n",
            "    zlib-1.2.13                |       hd590300_5          91 KB  conda-forge\n",
            "    zstandard-0.23.0           |  py310ha39cb0e_1         399 KB  conda-forge\n",
            "    zstd-1.5.6                 |       ha6fb4c9_0         542 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        1.61 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  absl-py            conda-forge/noarch::absl-py-2.1.0-pyhd8ed1ab_1 \n",
            "  alignn             conda-forge/noarch::alignn-2024.10.30-pyhd8ed1ab_0 \n",
            "  annotated-types    conda-forge/noarch::annotated-types-0.7.0-pyhd8ed1ab_1 \n",
            "  ase                conda-forge/noarch::ase-3.23.0-pyhd8ed1ab_0 \n",
            "  astunparse         conda-forge/noarch::astunparse-1.6.3-pyhd8ed1ab_2 \n",
            "  babel              conda-forge/noarch::babel-2.16.0-pyhd8ed1ab_1 \n",
            "  blinker            conda-forge/noarch::blinker-1.9.0-pyhff2d567_0 \n",
            "  brotli             conda-forge/linux-64::brotli-1.1.0-hd590300_1 \n",
            "  brotli-bin         conda-forge/linux-64::brotli-bin-1.1.0-hd590300_1 \n",
            "  cached-property    conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1 \n",
            "  cached_property    conda-forge/noarch::cached_property-1.5.2-pyha770c72_1 \n",
            "  click              conda-forge/noarch::click-8.1.8-pyh707e725_0 \n",
            "  contourpy          conda-forge/linux-64::contourpy-1.3.1-py310h3788b33_0 \n",
            "  cuda-cudart        nvidia/linux-64::cuda-cudart-12.4.127-0 \n",
            "  cuda-cupti         nvidia/linux-64::cuda-cupti-12.4.127-0 \n",
            "  cuda-libraries     nvidia/linux-64::cuda-libraries-12.4.1-0 \n",
            "  cuda-nvrtc         nvidia/linux-64::cuda-nvrtc-12.4.127-0 \n",
            "  cuda-nvtx          nvidia/linux-64::cuda-nvtx-12.4.127-0 \n",
            "  cuda-opencl        nvidia/linux-64::cuda-opencl-12.6.77-0 \n",
            "  cuda-runtime       nvidia/linux-64::cuda-runtime-12.4.1-0 \n",
            "  cuda-version       nvidia/noarch::cuda-version-12.6-3 \n",
            "  cycler             conda-forge/noarch::cycler-0.12.1-pyhd8ed1ab_1 \n",
            "  dgl                conda-forge/linux-64::dgl-2.1.0-py310h039f8d5_2 \n",
            "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0 \n",
            "  filelock           conda-forge/noarch::filelock-3.16.1-pyhd8ed1ab_1 \n",
            "  flake8             conda-forge/noarch::flake8-7.1.1-pyhd8ed1ab_1 \n",
            "  flask              conda-forge/noarch::flask-3.1.0-pyhff2d567_0 \n",
            "  flatbuffers        conda-forge/linux-64::flatbuffers-24.3.25-h59595ed_0 \n",
            "  fonttools          conda-forge/linux-64::fonttools-4.55.3-py310h89163eb_1 \n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
            "  fsspec             conda-forge/noarch::fsspec-2024.12.0-pyhd8ed1ab_0 \n",
            "  gast               conda-forge/noarch::gast-0.6.0-pyhd8ed1ab_0 \n",
            "  ghp-import         conda-forge/noarch::ghp-import-2.1.0-pyhd8ed1ab_2 \n",
            "  giflib             conda-forge/linux-64::giflib-5.2.2-hd590300_0 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
            "  gmpy2              conda-forge/linux-64::gmpy2-2.1.5-py310he8512ff_3 \n",
            "  gnutls             conda-forge/linux-64::gnutls-3.6.13-h85f3911_1 \n",
            "  google-pasta       conda-forge/noarch::google-pasta-0.2.0-pyhd8ed1ab_2 \n",
            "  grpcio             conda-forge/linux-64::grpcio-1.62.2-py310h1b8f574_0 \n",
            "  h5py               conda-forge/linux-64::h5py-3.12.1-nompi_py310h60e0fe6_103 \n",
            "  hdf5               conda-forge/linux-64::hdf5-1.14.3-nompi_hdf9ad27_105 \n",
            "  importlib-metadata conda-forge/noarch::importlib-metadata-8.5.0-pyha770c72_1 \n",
            "  importlib-resourc~ conda-forge/noarch::importlib-resources-6.4.5-pyhd8ed1ab_1 \n",
            "  importlib_resourc~ conda-forge/noarch::importlib_resources-6.4.5-pyhd8ed1ab_1 \n",
            "  inflect            conda-forge/noarch::inflect-7.5.0-pyhd8ed1ab_0 \n",
            "  itsdangerous       conda-forge/noarch::itsdangerous-2.2.0-pyhd8ed1ab_1 \n",
            "  jarvis-tools       conda-forge/noarch::jarvis-tools-2024.10.30-pyhd8ed1ab_0 \n",
            "  jinja2             conda-forge/noarch::jinja2-3.1.5-pyhd8ed1ab_0 \n",
            "  joblib             conda-forge/noarch::joblib-1.4.2-pyhd8ed1ab_1 \n",
            "  keras              conda-forge/noarch::keras-3.7.0-pyh753f3f9_1 \n",
            "  kiwisolver         conda-forge/linux-64::kiwisolver-1.4.7-py310h3788b33_0 \n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
            "  lcms2              conda-forge/linux-64::lcms2-2.16-hb7c19ff_0 \n",
            "  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n",
            "  libabseil          conda-forge/linux-64::libabseil-20240116.2-cxx17_he02047a_1 \n",
            "  libaec             conda-forge/linux-64::libaec-1.1.3-h59595ed_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-26_linux64_openblas \n",
            "  libbrotlicommon    conda-forge/linux-64::libbrotlicommon-1.1.0-hd590300_1 \n",
            "  libbrotlidec       conda-forge/linux-64::libbrotlidec-1.1.0-hd590300_1 \n",
            "  libbrotlienc       conda-forge/linux-64::libbrotlienc-1.1.0-hd590300_1 \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-26_linux64_openblas \n",
            "  libcublas          nvidia/linux-64::libcublas-12.4.5.8-0 \n",
            "  libcufft           nvidia/linux-64::libcufft-11.2.1.3-0 \n",
            "  libcufile          nvidia/linux-64::libcufile-1.11.1.6-0 \n",
            "  libcurand          nvidia/linux-64::libcurand-10.3.7.77-0 \n",
            "  libcusolver        nvidia/linux-64::libcusolver-11.6.1.9-0 \n",
            "  libcusparse        nvidia/linux-64::libcusparse-12.3.1.170-0 \n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.20-hd590300_0 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libgfortran        conda-forge/linux-64::libgfortran-14.2.0-h69a702a_1 \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-14.2.0-h69a702a_1 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-14.2.0-hd5240d6_1 \n",
            "  libgrpc            conda-forge/linux-64::libgrpc-1.62.2-h15f2491_0 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.9.3-default_h554bfaf_1009 \n",
            "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-3.0.0-hd590300_1 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-26_linux64_openblas \n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-26_linux64_openblas \n",
            "  libnpp             nvidia/linux-64::libnpp-12.2.5.30-0 \n",
            "  libnvfatbin        nvidia/linux-64::libnvfatbin-12.6.77-0 \n",
            "  libnvjitlink       nvidia/linux-64::libnvjitlink-12.4.127-0 \n",
            "  libnvjpeg          nvidia/linux-64::libnvjpeg-12.3.1.117-0 \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.28-pthreads_h94d23a6_1 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.43-h2797004_0 \n",
            "  libprotobuf        conda-forge/linux-64::libprotobuf-4.25.3-h08a7969_0 \n",
            "  libre2-11          conda-forge/linux-64::libre2-11-2023.09.01-h5a48ba9_2 \n",
            "  libstdcxx          conda-forge/linux-64::libstdcxx-14.2.0-hc0a3c3a_1 \n",
            "  libtiff            conda-forge/linux-64::libtiff-4.6.0-h1dd3fc0_3 \n",
            "  libtorch           conda-forge/linux-64::libtorch-2.3.1-cpu_mkl_h0bb0d08_100 \n",
            "  liburing           conda-forge/linux-64::liburing-2.6-h297d8ca_0 \n",
            "  libuv              conda-forge/linux-64::libuv-1.49.2-hb9d3cd8_0 \n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.5.0-h851e524_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-19.1.6-h024ca30_0 \n",
            "  markdown           conda-forge/noarch::markdown-3.6-pyhd8ed1ab_0 \n",
            "  markdown-it-py     conda-forge/noarch::markdown-it-py-3.0.0-pyhd8ed1ab_1 \n",
            "  markupsafe         conda-forge/linux-64::markupsafe-3.0.2-py310h89163eb_1 \n",
            "  matplotlib-base    conda-forge/linux-64::matplotlib-base-3.10.0-py310h68603db_0 \n",
            "  mccabe             conda-forge/noarch::mccabe-0.7.0-pyhd8ed1ab_1 \n",
            "  mdurl              conda-forge/noarch::mdurl-0.1.2-pyhd8ed1ab_1 \n",
            "  mergedeep          conda-forge/noarch::mergedeep-1.3.4-pyhd8ed1ab_1 \n",
            "  metis              conda-forge/linux-64::metis-5.1.1-h59595ed_2 \n",
            "  mkdocs             conda-forge/noarch::mkdocs-1.6.1-pyhd8ed1ab_1 \n",
            "  mkdocs-get-deps    conda-forge/noarch::mkdocs-get-deps-0.2.0-pyhd8ed1ab_1 \n",
            "  mkdocs-material    conda-forge/noarch::mkdocs-material-9.5.49-pyhd8ed1ab_0 \n",
            "  mkdocs-material-e~ conda-forge/noarch::mkdocs-material-extensions-1.3.1-pyhd8ed1ab_1 \n",
            "  mkl                conda-forge/linux-64::mkl-2023.2.0-h84fe81f_50496 \n",
            "  ml_dtypes          conda-forge/linux-64::ml_dtypes-0.3.2-py310hcc13569_0 \n",
            "  more-itertools     conda-forge/noarch::more-itertools-10.5.0-pyhd8ed1ab_1 \n",
            "  mpc                conda-forge/linux-64::mpc-1.3.1-h24ddda3_1 \n",
            "  mpfr               conda-forge/linux-64::mpfr-4.2.1-h90cbb55_3 \n",
            "  mpmath             conda-forge/noarch::mpmath-1.3.0-pyhd8ed1ab_1 \n",
            "  munkres            conda-forge/noarch::munkres-1.1.4-pyh9f0ad1d_0 \n",
            "  namex              conda-forge/noarch::namex-0.0.8-pyhd8ed1ab_1 \n",
            "  nettle             conda-forge/linux-64::nettle-3.6-he412f7d_0 \n",
            "  networkx           conda-forge/noarch::networkx-3.4.2-pyh267e887_2 \n",
            "  numpy              conda-forge/linux-64::numpy-1.26.4-py310hb13e2d6_0 \n",
            "  openh264           conda-forge/linux-64::openh264-2.1.1-h780b84a_0 \n",
            "  openjpeg           conda-forge/linux-64::openjpeg-2.5.2-h488ebb8_0 \n",
            "  opt_einsum         conda-forge/noarch::opt_einsum-3.4.0-pyhd8ed1ab_1 \n",
            "  optree             conda-forge/linux-64::optree-0.13.1-py310h3788b33_1 \n",
            "  paginate           conda-forge/noarch::paginate-0.5.7-pyhd8ed1ab_1 \n",
            "  pandas             conda-forge/linux-64::pandas-2.2.3-py310h5eaa309_1 \n",
            "  pathspec           conda-forge/noarch::pathspec-0.12.1-pyhd8ed1ab_1 \n",
            "  pillow             conda-forge/linux-64::pillow-10.3.0-py310hf73ecf8_0 \n",
            "  protobuf           conda-forge/linux-64::protobuf-4.25.3-py310h0e2eeba_1 \n",
            "  psutil             conda-forge/linux-64::psutil-6.1.1-py310ha75aee5_0 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-hb9d3cd8_1002 \n",
            "  pycodestyle        conda-forge/noarch::pycodestyle-2.12.1-pyhd8ed1ab_1 \n",
            "  pydantic           conda-forge/noarch::pydantic-2.10.4-pyh3cfb1c2_0 \n",
            "  pydantic-core      conda-forge/linux-64::pydantic-core-2.27.2-py310h505e2c1_0 \n",
            "  pydantic-settings  conda-forge/noarch::pydantic-settings-2.7.1-pyh3cfb1c2_0 \n",
            "  pydocstyle         conda-forge/noarch::pydocstyle-6.3.0-pyhd8ed1ab_1 \n",
            "  pyflakes           conda-forge/noarch::pyflakes-3.2.0-pyhd8ed1ab_1 \n",
            "  pygments           conda-forge/noarch::pygments-2.18.0-pyhd8ed1ab_1 \n",
            "  pymdown-extensions conda-forge/noarch::pymdown-extensions-10.13-pyhd8ed1ab_0 \n",
            "  pyparsing          conda-forge/noarch::pyparsing-2.4.7-pyhd8ed1ab_1 \n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.9.0.post0-pyhff2d567_1 \n",
            "  python-dotenv      conda-forge/noarch::python-dotenv-1.0.1-pyhd8ed1ab_1 \n",
            "  python-flatbuffers conda-forge/noarch::python-flatbuffers-24.3.25-pyhe33e51e_1 \n",
            "  python-lmdb        conda-forge/linux-64::python-lmdb-1.5.1-py310h74df5ed_1 \n",
            "  python-tzdata      conda-forge/noarch::python-tzdata-2024.2-pyhd8ed1ab_1 \n",
            "  pytorch            conda-forge/linux-64::pytorch-2.3.1-cpu_mkl_py310h75865b9_100 \n",
            "  pytorch-cuda       pytorch/linux-64::pytorch-cuda-12.4-hc786d27_7 \n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cpu \n",
            "  pytz               conda-forge/noarch::pytz-2024.1-pyhd8ed1ab_0 \n",
            "  pyyaml             conda-forge/linux-64::pyyaml-6.0.2-py310ha75aee5_1 \n",
            "  pyyaml-env-tag     conda-forge/noarch::pyyaml-env-tag-0.1-pyhd8ed1ab_1 \n",
            "  qhull              conda-forge/linux-64::qhull-2020.2-h434a139_5 \n",
            "  re2                conda-forge/linux-64::re2-2023.09.01-h7f4b329_2 \n",
            "  regex              conda-forge/linux-64::regex-2024.11.6-py310ha75aee5_0 \n",
            "  rich               conda-forge/noarch::rich-13.9.4-pyhd8ed1ab_1 \n",
            "  scikit-learn       conda-forge/linux-64::scikit-learn-1.6.0-py310h27f47ee_0 \n",
            "  scipy              conda-forge/linux-64::scipy-1.14.1-py310hfcf56fc_2 \n",
            "  six                conda-forge/noarch::six-1.17.0-pyhd8ed1ab_0 \n",
            "  sleef              conda-forge/linux-64::sleef-3.7-h1b44611_2 \n",
            "  snappy             conda-forge/linux-64::snappy-1.2.1-h8bd8927_1 \n",
            "  snowballstemmer    conda-forge/noarch::snowballstemmer-2.2.0-pyhd8ed1ab_0 \n",
            "  spglib             conda-forge/linux-64::spglib-2.5.0-py310h1e9006d_3 \n",
            "  sympy              conda-forge/noarch::sympy-1.13.3-pypyh2585a3b_103 \n",
            "  tbb                conda-forge/linux-64::tbb-2021.11.0-h00ab1b0_1 \n",
            "  tensorboard        conda-forge/noarch::tensorboard-2.16.2-pyhd8ed1ab_0 \n",
            "  tensorboard-data-~ conda-forge/linux-64::tensorboard-data-server-0.7.0-py310h6c63255_2 \n",
            "  tensorflow         conda-forge/linux-64::tensorflow-2.16.1-cpu_py310h49b650b_0 \n",
            "  tensorflow-base    conda-forge/linux-64::tensorflow-base-2.16.1-cpu_py310h224022f_0 \n",
            "  tensorflow-estima~ conda-forge/linux-64::tensorflow-estimator-2.16.1-cpu_py310hc6dcfef_0 \n",
            "  termcolor          conda-forge/noarch::termcolor-2.5.0-pyhd8ed1ab_1 \n",
            "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.5.0-pyhc1e730c_0 \n",
            "  tomli              conda-forge/noarch::tomli-2.2.1-pyhd8ed1ab_1 \n",
            "  toolz              conda-forge/noarch::toolz-1.0.0-pyhd8ed1ab_1 \n",
            "  torchaudio         pytorch/linux-64::torchaudio-2.3.1-py310_cpu \n",
            "  torchdata          pytorch/linux-64::torchdata-0.9.0-py310 \n",
            "  torchvision        pytorch/linux-64::torchvision-0.18.1-py310_cpu \n",
            "  typeguard          conda-forge/noarch::typeguard-4.4.1-pyhd8ed1ab_1 \n",
            "  typing-extensions  conda-forge/noarch::typing-extensions-4.12.2-hd8ed1ab_1 \n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.12.2-pyha770c72_1 \n",
            "  unicodedata2       conda-forge/linux-64::unicodedata2-15.1.0-py310ha75aee5_1 \n",
            "  watchdog           conda-forge/linux-64::watchdog-6.0.0-py310hff52083_0 \n",
            "  werkzeug           conda-forge/noarch::werkzeug-3.1.3-pyhd8ed1ab_1 \n",
            "  wrapt              conda-forge/linux-64::wrapt-1.17.0-py310ha75aee5_0 \n",
            "  xmltodict          conda-forge/noarch::xmltodict-0.14.2-pyhd8ed1ab_1 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.12-hb9d3cd8_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.5-hb9d3cd8_0 \n",
            "  yaml               conda-forge/linux-64::yaml-0.2.5-h7f98852_2 \n",
            "  zipp               conda-forge/noarch::zipp-3.21.0-pyhd8ed1ab_1 \n",
            "  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  c-ares                                  1.24.0-hd590300_0 --> 1.34.4-hb9d3cd8_0 \n",
            "  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.12.14-hbcca054_0 \n",
            "  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2024.12.14-pyhd8ed1ab_0 \n",
            "  libcurl                                  8.5.0-hca28451_0 --> 8.8.0-hca28451_0 \n",
            "  libgcc-ng                               13.2.0-h807b86a_3 --> 14.2.0-h69a702a_1 \n",
            "  libgomp                                 13.2.0-h807b86a_3 --> 14.2.0-h77fa898_1 \n",
            "  libsqlite                               3.44.2-h2797004_0 --> 3.46.0-hde9e2c9_0 \n",
            "  openssl                                  3.2.0-hd590300_1 --> 3.4.0-hb9d3cd8_0 \n",
            "  zstandard                          0.22.0-py310h1275a96_0 --> 0.23.0-py310ha39cb0e_1 \n",
            "  zstd                                     1.5.5-hfc55251_0 --> 1.5.6-ha6fb4c9_0 \n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-2_gnu --> 4.5-2_kmp_llvm \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "CPU times: user 2.55 s, sys: 366 ms, total: 2.92 s\n",
            "Wall time: 6min 8s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!conda install alignn dgl=2.1.0 pytorch torchvision torchaudio pytorch-cuda -c pytorch -c nvidia -y --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and pre-processing the data\n"
      ],
      "metadata": {
        "id": "DmL4_32u84Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always better to standardize and normalize the feature and target values before any ML exercise. The following methodology can be used in the same order for any kind of raw data that is available on any materials data repository.\n",
        "\n",
        "In the following exercise we will take up the formation energy (in eV/atom) and highest optical mode phonon frequency (in $cm^{-1}$) datasets from the MatBench repository.\n",
        "\n",
        "1.   We split the raw data into train and test dataset in the ratio 90:10\n",
        "2.   We standardize the target values of both train and test set using the StandardScaler class from sklearn (ref: https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html) using only the training data statistics\n",
        "     \n",
        "     **z = (x - u) / s**\n",
        "\n",
        "          z => Standard score\n",
        "          x => Sample\n",
        "          u => Mean\n",
        "          s => Standard deviation\n",
        "3.   We then normalize the values between -1 and +1\n",
        "\n",
        "     **x$_{norm}$ = a + (((x - x$_{minimum}$) * (b - a)) / range of x)**\n",
        "     \n",
        "          a, b => Customized range to normalize (say -1 to +1)\n",
        "4.   We choose 200 random points from the training dataset for illustration\n",
        "\n",
        "\n",
        "**Make sure to use the training data statistics for standardizing and normalizing both the train and the test set**"
      ],
      "metadata": {
        "id": "G5zcUkNOBNei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data"
      ],
      "metadata": {
        "id": "kpWoWrlE9IV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzHd2mwUO_SQ",
        "outputId": "af9944e4-1bb1-4ff8-df64-4962b3f2a9f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1iXEihej_LQgTLPqOk66oPV3o01Ze7SBp current_model.pt\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iXEihej_LQgTLPqOk66oPV3o01Ze7SBp\n",
            "To: /content/form_energy/current_model.pt\n",
            "100% 1.17M/1.17M [00:00<00:00, 128MB/s]\n",
            "Download completed\n",
            "Retrieving folder contents\n",
            "Processing file 1Zhs94rpJ8bcM7HBXeg2yx7ifi59oG6xH TEST-UNSTD.json\n",
            "Processing file 1FKTGiwFOsMD8KlHH-OuD-QBYrD0ApUYx TRAIN-UNSTD.json\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zhs94rpJ8bcM7HBXeg2yx7ifi59oG6xH\n",
            "To: /content/phonons/TEST-UNSTD.json\n",
            "100% 76.5k/76.5k [00:00<00:00, 78.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FKTGiwFOsMD8KlHH-OuD-QBYrD0ApUYx\n",
            "To: /content/phonons/TRAIN-UNSTD.json\n",
            "100% 683k/683k [00:00<00:00, 91.7MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1LupaRopHz5v0yEyoxzn0j2CWbwCZVrr_?usp=sharing\n",
        "!gdown --folder https://drive.google.com/drive/folders/1PO7eyZHc8WOHsuYUlYB1Cj1OSR-wJrZA?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "2aEOlOZK9WDh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWX7nhTVUILq"
      },
      "source": [
        "\n",
        "Load the json file containing the data for bandgap prediction.\n",
        "Notice that each entry is a dictionary with three important details corresponding to the datapoint\n",
        "\n",
        "1.   a \"ID\" named as 'jid'\n",
        "2.   Target value named as 'target'\n",
        "3.   The structure saved in jarvis \"Atoms\" format saved as \"atoms\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN-qVii0bzd-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "with open('/content/phonons/TRAIN-UNSTD.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "with open('/content/phonons/TEST-UNSTD.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "df_train = pd.DataFrame.from_dict(train_data)\n",
        "df_test = pd.DataFrame.from_dict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCVrgFnWbo09"
      },
      "outputs": [],
      "source": [
        "# test_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardize and Normalize the data"
      ],
      "metadata": {
        "id": "tUhSCOBD9gT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_av8L1-hVATx"
      },
      "outputs": [],
      "source": [
        "#**Standardize**\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_arr = scaler.fit_transform(df_train[['target']]).flatten()\n",
        "test_arr = scaler.transform(df_test[['target']]).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IlBxl7CG_22"
      },
      "outputs": [],
      "source": [
        "# explicit function to normalize array\n",
        "def normalize(train_arr,test_arr, t_min, t_max):\n",
        "    norm_arr_train = []\n",
        "    diff = t_max - t_min\n",
        "    diff_arr = max(train_arr) - min(train_arr)\n",
        "    for i in train_arr:\n",
        "        temp = (((i - min(train_arr))*diff)/diff_arr) + t_min\n",
        "        norm_arr_train.append(temp)\n",
        "    norm_arr_test = []\n",
        "    for i in test_arr:\n",
        "        temp = (((i - min(train_arr))*diff)/diff_arr) + t_min\n",
        "        norm_arr_test.append(temp)\n",
        "    return norm_arr_train, norm_arr_test\n",
        "\n",
        "range_to_normalize = (-1, 1)\n",
        "norm_train, norm_test = normalize(train_arr,test_arr, range_to_normalize[0],range_to_normalize[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the data\n",
        "\n",
        "Visualize the distributions before and after standardization and normalization"
      ],
      "metadata": {
        "id": "xNNDsdym9mSd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "9UVIoxrnVWqx",
        "outputId": "cdd531a0-48b5-41c8-948c-088184b47869"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABv8UlEQVR4nO3dd3RU1fr/8c8kpIeEkoQQSui9KTUoRUFCsSBYEJCAiIoUkaLg5Uu/F0VEFCkWCOgVURSxIUhXERERpIVQBKJCwFACIRBIsn9/8MtchiSQMicT4P1aa9bKnLNnn2efczLPPDOn2IwxRgAAAAAAwOncXB0AAAAAAAA3K4puAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuwELjxo2TzWYrkGWtW7dONptN69ats0/r3bu3KlSoUCDLz3Do0CHZbDbNnz+/QJd7La1bt1br1q1dHYZLsX8AuFlVqFBBvXv3LpBlZfW+abPZNG7cuAJZfoaC/HyRE7y3X8b+gexQdCNH5s+fL5vNJm9vb/3999+Z5rdu3Vp16tTJU9+zZs3K1Zt0UlKSxo4dqzp16sjPz08lS5ZUgwYN9Nxzz+nIkSP2dsuWLSvwNznc2CpUqCCbzaZBgwZlmpdRtH766acuiAwACtaOHTv00EMPKTw8XN7e3ipTpozuuecezZgxw6Hdf/7zHy1dutQ1QeKGk5FLbTabtmzZkml+79695e/v74LIAGtRdCNXUlJS9PLLLzu1z9wU3ZcuXVLLli316quvqkWLFpo2bZpeeukl3X777Vq4cKH27t1rb7ts2TKNHz/eqbHeaN59913Fxsa6OowbzrvvvuvwBc7Niv0DQFZ++uknNWrUSL///rv69eunt956S08++aTc3Nz0xhtvOLSl6JbOnz+v0aNHuzqMG86t8sMI+wckqYirA8CNpUGDBnr33Xc1atQohYWFFfjyly5dqq1bt+rDDz9U9+7dHeZduHBBFy9eLPCYCooxRhcuXJCPj0+OX+Ph4WFhRDen2rVrKzY2Vi+//LLefPNNy5Zz7tw5+fn5WdZ/TrB/AMjKv//9bwUGBmrz5s0qVqyYw7zjx4+7JqgCcuHCBXl6esrNLee/S3l7e1sY0c2pQYMG+vrrr/Xbb7/p9ttvt2w5ycnJ8vX1taz/nGD/gMQv3cill156SWlpaTn6tTs1NVUTJ05U5cqV5eXlpQoVKuill15SSkqKvU2FChW0a9curV+/3n640bXOvT1w4IAk6Y477sg0z9vbWwEBAZIuH540c+ZMSbL3e+W5LVOnTlXz5s1VsmRJ+fj4qGHDhlkeNmyz2TRw4EAtXbpUderUkZeXl2rXrq3ly5dnavvjjz+qcePG8vb2VuXKlfX2229nOYbo6GjdfffdCgkJkZeXl2rVqqXZs2dnalehQgXde++9WrFihRo1aiQfHx97n3/99Zc6d+4sPz8/hYSE6Pnnn3dYrxmuPreodevWDuvjyseVRxucPn1aQ4YMUbly5eTl5aUqVarolVdeUXp6ukP/p0+fVu/evRUYGKhixYopKipKp0+fznLcVzt58qSGDx+uunXryt/fXwEBAerQoYN+//13h3YZh6J98skn+ve//62yZcvK29tbbdq00f79+zP1+84776hy5cry8fFRkyZN9MMPP+QongwVKlRQr169cvxr99atW9WhQwcFBATI399fbdq00c8//+zQJuP0jPXr1+vZZ59VSEiIypYtK+l/p2Zs375drVq1kq+vr6pUqWLfH9evX6+mTZvKx8dH1atX16pVqxz6Pnz4sJ599llVr15dPj4+KlmypB5++GEdOnTourEX5v0DgOscOHBAtWvXzlRwS1JISIj9b5vNpnPnzmnBggX294qMc6tz+t6U8f64YcMGDR06VMHBwfLz89ODDz6of/75x6GtMUaTJk1S2bJl5evrq7vuuku7du3KFGNu88uiRYs0evRolSlTRr6+vjpz5owk2XO/t7e36tSpo88//zzL9XXlObsZ5zZn97jSpk2b1L59ewUGBsrX11etWrXShg0bMvWf088XWfnhhx/08MMPq3z58vLy8lK5cuX0/PPP6/z58w7tMg7r/vvvv9W5c2f5+/srODhYw4cPV1pamkNbZ7y3Dxo0SMWLF8/xr92zZs1S7dq15eXlpbCwMA0YMCDTMjPy6ZYtW9SyZUv5+vrqpZdesm+TqVOnaubMmapUqZJ8fX3Vrl07/fnnnzLGaOLEiSpbtqx8fHz0wAMP6OTJkw59f/HFF+rUqZPCwsLk5eWlypUra+LEiZnWTVYK8/6BgsMv3ciVihUr2guSkSNHXvPX7ieffFILFizQQw89pGHDhmnTpk2aPHmyYmJi7Ilr+vTpGjRokPz9/fWvf/1LklSqVKls+wwPD5ckvf/++xo9enS2F4l4+umndeTIEa1cuVIffPBBpvlvvPGG7r//fvXo0UMXL17UokWL9PDDD+vrr79Wp06dHNr++OOPWrJkiZ599lkVLVpUb775prp27aq4uDiVLFlS0uVz39q1a6fg4GCNGzdOqampGjt2bJZjmT17tmrXrq37779fRYoU0VdffaVnn31W6enpGjBggEPb2NhYPfbYY3r66afVr18/Va9eXefPn1ebNm0UFxenwYMHKywsTB988IHWrFmT7XrL8K9//UtPPvmkw7T//ve/WrFihf2DVHJyslq1aqW///5bTz/9tMqXL6+ffvpJo0aN0tGjRzV9+nRJlz/8PPDAA/rxxx/1zDPPqGbNmvr8888VFRV13Tgk6Y8//tDSpUv18MMPq2LFijp27JjefvtttWrVSrt37860b7388styc3PT8OHDlZiYqClTpqhHjx7atGmTvc3cuXP19NNPq3nz5hoyZIj++OMP3X///SpRooTKlSuXo7gy1tP7779/3V+7d+3apRYtWiggIEAvvPCCPDw89Pbbb6t169b2YvlKzz77rIKDgzVmzBidO3fOPv3UqVO699571a1bNz388MOaPXu2unXrpg8//FBDhgzRM888o+7du+vVV1/VQw89pD///FNFixaVJG3evFk//fSTunXrprJly+rQoUOaPXu2Wrdurd27d+fqG/7CtH8AcJ3w8HBt3LhRO3fuvOb1Wj744AM9+eSTatKkiZ566ilJUuXKlSXl/r0powgbO3asDh06pOnTp2vgwIH6+OOP7W3GjBmjSZMmqWPHjurYsaN+++03tWvXLtNRbrnNLxMnTpSnp6eGDx+ulJQUeXp66rvvvlPXrl1Vq1YtTZ48WSdOnFCfPn3sX5hmJzg4ONPnjkuXLun555+Xp6enfdqaNWvUoUMHNWzYUGPHjpWbm5v9S/kffvhBTZo0kZS7zxdZWbx4sZKTk9W/f3+VLFlSv/zyi2bMmKG//vpLixcvdmiblpamyMhINW3aVFOnTtWqVav02muvqXLlyurfv78k5723BwQE6Pnnn9eYMWOu+2v3uHHjNH78eLVt21b9+/dXbGysZs+erc2bN2vDhg0OR22dOHFCHTp0ULdu3dSzZ0+H9fThhx/q4sWLGjRokE6ePKkpU6bokUce0d13361169bpxRdf1P79+zVjxgwNHz5c8+bNs792/vz58vf319ChQ+Xv7681a9ZozJgxOnPmjF599dUcj7uw7R8oQAbIgejoaCPJbN682Rw4cMAUKVLEDB482D6/VatWpnbt2vbn27ZtM5LMk08+6dDP8OHDjSSzZs0a+7TatWubVq1a5SiO5ORkU716dSPJhIeHm969e5u5c+eaY8eOZWo7YMAAk90unpyc7PD84sWLpk6dOubuu+92mC7JeHp6mv3799un/f7770aSmTFjhn1a586djbe3tzl8+LB92u7du427u3umGK5etjHGREZGmkqVKjlMCw8PN5LM8uXLHaZPnz7dSDKffPKJfdq5c+dMlSpVjCSzdu1a+/SoqCgTHh6e5TowxpgNGzYYDw8P88QTT9inTZw40fj5+Zm9e/c6tB05cqRxd3c3cXFxxhhjli5daiSZKVOm2NukpqaaFi1aGEkmOjo62+UaY8yFCxdMWlqaw7SDBw8aLy8vM2HCBPu0tWvXGkmmZs2aJiUlxT79jTfeMJLMjh07jDGXt2FISIhp0KCBQ7t33nnHSMrRPhYeHm46depkjDGmT58+xtvb2xw5csQhjsWLF9vbd+7c2Xh6epoDBw7Ypx05csQULVrUtGzZ0j4t4//nzjvvNKmpqQ7LbNWqlZFkFi5caJ+2Z88eI8m4ubmZn3/+2T59xYoVmdZtVvvTxo0bjSTz/vvv26dlxH+j7B8AXOe7774z7u7uxt3d3URERJgXXnjBrFixwly8eDFTWz8/PxMVFZVpek7fmzLeH9u2bWvS09Pt059//nnj7u5uTp8+bYwx5vjx48bT09N06tTJod1LL71kJDnEkNv8UqlSpUzxNmjQwJQuXdq+/Iz1kvH540qSzNixYzONN8Ozzz5r3N3d7Z990tPTTdWqVU1kZKTDWJKTk03FihXNPffcY5+Wm88XWclqO0yePNnYbDaHPqOioowkh/VjjDG33Xabadiwof15ft/br8ylp0+fNsWLFzf333+/Qxx+fn725xnbvV27dg7b9K233jKSzLx58+zTMvLpnDlzHJZ58OBBI8kEBwc7bM9Ro0YZSaZ+/frm0qVL9umPPfaY8fT0NBcuXLBPy2o9Pv3008bX19ehXVZ5tTDvHyg4HF6OXKtUqZIef/xxvfPOOzp69GiWbZYtWyZJGjp0qMP0YcOGSZK++eabPC3bx8dHmzZt0ogRIyRd/uaxb9++Kl26tAYNGpTlIdbZ9ZPh1KlTSkxMVIsWLfTbb79latu2bVv7N/eSVK9ePQUEBOiPP/6QdPmb4RUrVqhz584qX768vV3NmjUVGRl5zWUnJiYqISFBrVq10h9//KHExESHthUrVszUx7Jly1S6dGk99NBD9mm+vr72XxlyKj4+Xg899JAaNGigWbNm2acvXrxYLVq0UPHixZWQkGB/tG3bVmlpafr+++/tcRQpUsT+7bckubu7Z3nl76x4eXnZz5lLS0vTiRMn5O/vr+rVq2e5Hfr06ePwLXCLFi0kyb4dfv31Vx0/flzPPPOMQ7uMQ+Bya/To0UpNTc32VIq0tDR999136ty5sypVqmSfXrp0aXXv3l0//vij/RDFDP369ZO7u3umvvz9/dWtWzf78+rVq6tYsWKqWbOmw6/lGX9njFly3J8uXbqkEydOqEqVKipWrFiW6zGnXL1/AHCde+65Rxs3btT999+v33//XVOmTFFkZKTKlCmjL7/8Mkd95Pa96amnnnI4eq1FixZKS0vT4cOHJUmrVq2y/0p5ZbshQ4Zk6iu3+SUqKsoh3qNHj2rbtm2KiopyyB/33HOPatWqlaPxZ3j//fc1a9YsTZkyRXfddZckadu2bdq3b5+6d++uEydO2N9Hz507pzZt2uj7779Xenp6rj9fZOXKcZ07d04JCQlq3ry5jDHaunVrpvbPPPOMw/MWLVo45BxnvrcHBgZqyJAh+vLLL7OMRfrfdh8yZIjDefb9+vVTQEBAps+TXl5e6tOnT5Z9Pfzwww7bMyOn9uzZU0WKFHGYfvHiRYe79Vy5Hs+ePauEhAS1aNFCycnJ2rNnTy5G7cjV+wcKDkU38uR6Bcnhw4fl5uamKlWqOEwPDQ1VsWLF7Ek0LwIDAzVlyhQdOnRIhw4d0ty5c1W9enW99dZbmjhxYo76+Prrr9WsWTN5e3urRIkSCg4O1uzZszMVvZIc3sgyFC9eXKdOnZIk/fPPPzp//ryqVq2aqV316tUzTduwYYPatm0rPz8/FStWTMHBwXrppZckKcui+2qHDx9WlSpVMh1an9WyspOamqpHHnlEaWlpWrJkiby8vOzz9u3bp+XLlys4ONjh0bZtW0n/u4jO4cOHVbp06Uy39shpHOnp6Xr99ddVtWpVeXl5KSgoSMHBwdq+fXuOtkPx4sUlyb4dMvapq7eDh4eHQ1GcU9f7cumff/5RcnJyluOtWbOm0tPT9eeffzpMz2p7SlLZsmUzbc/AwMBMh8RnfFjIGLN0+aqoY8aMsZ9fnbEeT58+neV6zInCsH8AcK3GjRtryZIlOnXqlH755ReNGjVKZ8+e1UMPPaTdu3df9/W5fW/K63t8cHCwvW2G3OaXq9+bs1uWlLv3sG3btumZZ57RY4895vAjxL59+yRdLvavfi997733lJKSosTExFx/vshKXFycevfurRIlStjP027VqpWkzJ85vL29FRwc7DDtys87kvPf25977jkVK1Ys23O7M7bF1f17enqqUqVKmT5PlilTxuGL9ytdvY9l5NSc5Npdu3bpwQcfVGBgoAICAhQcHKyePXtKyrwec6ow7B8oOJzTjTypVKmSevbsqXfeeUcjR47Mtl1251w7S3h4uJ544gk9+OCDqlSpkj788ENNmjTpmq/54YcfdP/996tly5aaNWuWSpcuLQ8PD0VHR2vhwoWZ2mf1y6R0+bym3Dpw4IDatGmjGjVqaNq0aSpXrpw8PT21bNkyvf7665kuRJWbK5XnxogRI7Rx40atWrUq0/lp6enpuueee/TCCy9k+dpq1ao5JYb//Oc/+r//+z898cQTmjhxokqUKCE3NzcNGTIk03qQnLsdcupf//qXPvjgA73yyivq3LlzvvvLbntmN7acjHnQoEGKjo7WkCFDFBERocDAQNlsNnXr1i3L9ZgThWH/AFA4eHp6qnHjxmrcuLGqVaumPn36aPHixRo7duw1X5fb9yZnvsfnNr9YkWtPnTqlrl27qlq1anrvvfcc5mXE8Oqrr6pBgwZZvt7f3z/HR+9lJy0tTffcc49OnjypF198UTVq1JCfn5/+/vtv9e7dO9O6yG4bWCnj1+5x48Zl+2t3blxrW+Y1154+fVqtWrVSQECAJkyYoMqVK8vb21u//fabXnzxxTzl2sKwf6BgUXQjz0aPHq3//ve/euWVVzLNCw8PV3p6uvbt26eaNWvapx87dkynT5+2XxBNck5hXrx4cVWuXFk7d+68br+fffaZvL29tWLFCodf8KKjo/O07ODgYPn4+Ni/mbzS1fdA/uqrr5SSkqIvv/zS4RvXtWvX5nh54eHh2rlzp4wxDmPM6f2WFy1apOnTp2v69On2b7uvVLlyZSUlJdl/ubxWHKtXr1ZSUpLDN945jePTTz/VXXfdpblz5zpMP336tIKCgnLUx9XxSJe/Ib777rvt0y9duqSDBw+qfv36ue6zcuXK6tmzp95+++1MF0ULDg6Wr69vluPds2eP3NzccnXxtrz69NNPFRUVpddee80+7cKFC3m+Snhh2T8AFD6NGjWSJIejf7LLtc5+b7ryPf7Ko5f++ecfh18kM5adn/xy5bKulpP3sPT0dPXo0UOnT5/WqlWrMl00LuOUtYCAgGu+l+bm80VWduzYob1792rBggXq1auXffrKlSuv+9rsWPHePmTIEE2fPl3jx4/PdMX8jG0RGxvrsN0vXryogwcPXjcXOcO6det04sQJLVmyRC1btrRPP3jwYJ76Kyz7BwoWh5cjz64sSOLj4x3mdezYUZLsVzLOMG3aNElyuEK4n59fjpPw77//roSEhEzTDx8+rN27dzscTpNxD+Sr+3Z3d5fNZnO4zcOhQ4e0dOnSHMVwNXd3d0VGRmrp0qWKi4uzT4+JidGKFSsytZUcv7lPTEzMVcHfsWNHHTlyxOEWZ8nJyXrnnXeu+9qdO3fqySefVM+ePfXcc89l2eaRRx7Rxo0bM8UuXV6Xqamp9jhSU1MdbneWlpamGTNm5Ggc7u7umX7BWLx4scM5VLnRqFEjBQcHa86cOQ5Xsp0/f36+blM1evRoXbp0SVOmTHGY7u7urnbt2umLL75wuAXOsWPHtHDhQt155532W9hZKav1OGPGjBzdxuRqhWn/AOA6a9euzfIX5ozrtVyda7N6j3Xme5N0+foqHh4emjFjhkO/V3/OyG7ZuckvpUuXVoMGDbRgwQKHQ4dXrlyZo0Prx48frxUrVuijjz7K8rSihg0bqnLlypo6daqSkpIyzc+4VVpuPl9kJavPHMYYvfHGG9d9bXaseG/P+LX7iy++0LZt2xzmtW3bVp6ennrzzTcdxjF37lwlJiZmuuOMFbJajxcvXnS43kluFJb9AwWLX7qRLxmH38bGxqp27dr26fXr11dUVJTeeecd+2E5v/zyixYsWKDOnTvbLxYhXX5zmT17tiZNmqQqVaooJCTE4ZfKK61cuVJjx47V/fffr2bNmsnf319//PGH5s2bp5SUFIdzgho2bChJGjx4sCIjI+Xu7q5u3bqpU6dOmjZtmtq3b6/u3bvr+PHjmjlzpqpUqaLt27fnaT2MHz9ey5cvV4sWLfTss88qNTVVM2bMUO3atR36bNeunTw9PXXffffp6aefVlJSkt59912FhIRke1G6q/Xr109vvfWWevXqpS1btqh06dL64IMPcnRrqIyLi7Rs2VL//e9/HeY1b95clSpV0ogRI/Tll1/q3nvvVe/evdWwYUOdO3dOO3bs0KeffqpDhw4pKChI9913n+644w6NHDlShw4dUq1atbRkyZIcn9t07733asKECerTp4+aN2+uHTt26MMPP8zT+dfS5XO3J02apKefflp33323Hn30UR08eFDR0dF57lP635dLCxYsyDRv0qRJWrlype688049++yzKlKkiN5++22lpKRkKtKtcu+99+qDDz5QYGCgatWqZT8sPON2drlRmPYPAK4zaNAgJScn68EHH1SNGjV08eJF/fTTT/r4449VoUIFhwtVNWzYUKtWrdK0adMUFhamihUrqmnTpk59b5Jkv2f05MmTde+996pjx47aunWrvv3220y/Xjsjv0yePFmdOnXSnXfeqSeeeEInT5605/WsCqEMO3bs0MSJE9WyZUsdP34803tpz5495ebmpvfee08dOnRQ7dq11adPH5UpU0Z///231q5dq4CAAH311VeScv75Iis1atRQ5cqVNXz4cP39998KCAjQZ599lunIgNyw6r39ueee0+uvv67ff//d/qOJdHm7jxo1SuPHj1f79u11//33KzY2VrNmzVLjxo3t51VbqXnz5ipevLiioqI0ePBg2Ww2ffDBB3k69aEw7R8oYAV8tXTcoK68ZdjVMm4zceUtw4wx5tKlS2b8+PGmYsWKxsPDw5QrV86MGjXK4dYKxhgTHx9vOnXqZIoWLXrdWzv98ccfZsyYMaZZs2YmJCTEFClSxAQHB5tOnTo53IbMmMu3sBg0aJAJDg42NpvN4dYJc+fONVWrVjVeXl6mRo0aJjo62owdOzbT7RUkmQEDBmSKIzw8PNMtUtavX28aNmxoPD09TaVKlcycOXOy7PPLL7809erVM97e3qZChQrmlVdeMfPmzTOSzMGDBx2WkXH7qqsdPnzY3H///cbX19cEBQWZ5557zixfvvy6t4TKuA1ZVo8rb/Nx9uxZM2rUKFOlShXj6elpgoKCTPPmzc3UqVMdbhlz4sQJ8/jjj5uAgAATGBhoHn/8cbN169Yc3zJs2LBhpnTp0sbHx8fccccdZuPGjaZVq1YO+0BWt+oy5n+3ALl6ObNmzTIVK1Y0Xl5eplGjRub777/P1Gd2slvn+/bts99+4+o4fvvtNxMZGWn8/f2Nr6+vueuuu8xPP/3k0OZa/z9X327verFcvU+eOnXK9OnTxwQFBRl/f38TGRlp9uzZk2kfzcktwwrT/gHAdb799lvzxBNPmBo1ahh/f3/j6elpqlSpYgYNGpTpFp179uwxLVu2ND4+Pg637srpe1N2749ZvWelpaWZ8ePH2/NG69atzc6dOzP1md/8kuGzzz4zNWvWNF5eXqZWrVpmyZIl170lVEaf2T2utHXrVtOlSxdTsmRJ4+XlZcLDw80jjzxiVq9e7dAup58vsrJ7927Ttm1b4+/vb4KCgky/fv3stz698n346lt1ZchqOfl5b7/WOs9YVlZxvPXWW6ZGjRrGw8PDlCpVyvTv39+cOnXKoU12+TTj88Krr76ao1iy2ic3bNhgmjVrZnx8fExYWJj9NnrXy6vGFO79AwXHZoyFVyECAAAAAOAWxjndAAAAAABYhKIbAAAAAACLUHQDAAAAAGARim4AAAAAACxC0Q0AAAAAgEUougEAAAAAsEgRVwdQGKSnp+vIkSMqWrSobDabq8MBANzijDE6e/aswsLC5ObG9+MZyNcAgMIkp/maolvSkSNHVK5cOVeHAQCAgz///FNly5Z1dRiFBvkaAFAYXS9fU3RLKlq0qKTLKysgIMDF0QAAbnVnzpxRuXLl7PkJl5GvAQCFSU7zNUW3ZD9ELSAggCQOACg0OITaEfkaAFAYXS9fc6IYAAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARVxadM+ePVv16tVTQECAAgICFBERoW+//dY+/8KFCxowYIBKliwpf39/de3aVceOHXPoIy4uTp06dZKvr69CQkI0YsQIpaamFvRQAAC4aZGvAQDIuyKuXHjZsmX18ssvq2rVqjLGaMGCBXrggQe0detW1a5dW88//7y++eYbLV68WIGBgRo4cKC6dOmiDRs2SJLS0tLUqVMnhYaG6qefftLRo0fVq1cveXh46D//+Y8rh+Y0cXFxSkhIcEpfQUFBKl++vFP6AgDcOsjXOUPOBgBkyRQyxYsXN++99545ffq08fDwMIsXL7bPi4mJMZLMxo0bjTHGLFu2zLi5uZn4+Hh7m9mzZ5uAgACTkpKS42UmJiYaSSYxMdF5A3GCw4cPGx9fXyPJKQ8fX19z+PBhVw8LAHAdhTUvXYl87ejw4cPG18fHaTnb18eHnA0AhVxO85JLf+m+UlpamhYvXqxz584pIiJCW7Zs0aVLl9S2bVt7mxo1aqh8+fLauHGjmjVrpo0bN6pu3boqVaqUvU1kZKT69++vXbt26bbbbstyWSkpKUpJSbE/P3PmjHUDy4eEhASdT07WI5NmK6Ri1Xz1dfzgPn0yur8SEhL45hwAkGfk66wlJCQo+fx5vdOli6oFBeWrr70JCXpqyRJyNgDcJFxedO/YsUMRERG6cOGC/P399fnnn6tWrVratm2bPD09VaxYMYf2pUqVUnx8vCQpPj7eIYFnzM+Yl53Jkydr/Pjxzh2IhUIqVlWZmvVdHQYA4BZGvs6ZakFBahAW5uowAACFiMuvXl69enVt27ZNmzZtUv/+/RUVFaXdu3dbusxRo0YpMTHR/vjzzz8tXR4AADc68jUAAHnj8l+6PT09VaVKFUlSw4YNtXnzZr3xxht69NFHdfHiRZ0+fdrh2/Njx44pNDRUkhQaGqpffvnFob+Mq6VmtMmKl5eXvLy8nDwSAABuXuRrAADyxuW/dF8tPT1dKSkpatiwoTw8PLR69Wr7vNjYWMXFxSkiIkKSFBERoR07duj48eP2NitXrlRAQIBq1apV4LEDAHCrIF8DAJAzLv2le9SoUerQoYPKly+vs2fPauHChVq3bp1WrFihwMBA9e3bV0OHDlWJEiUUEBCgQYMGKSIiQs2aNZMktWvXTrVq1dLjjz+uKVOmKD4+XqNHj9aAAQP4ZhwAACchXwMAkHcuLbqPHz+uXr166ejRowoMDFS9evW0YsUK3XPPPZKk119/XW5uburatatSUlIUGRmpWbNm2V/v7u6ur7/+Wv3791dERIT8/PwUFRWlCRMmuGpIAADcdMjXAADknUuL7rlz515zvre3t2bOnKmZM2dm2yY8PFzLli1zdmgAAOD/I18DAJB3he6cbgAAAAAAbhYU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEZcW3ZMnT1bjxo1VtGhRhYSEqHPnzoqNjXVo07p1a9lsNofHM88849AmLi5OnTp1kq+vr0JCQjRixAilpqYW5FAAALhpka8BAMi7Iq5c+Pr16zVgwAA1btxYqampeumll9SuXTvt3r1bfn5+9nb9+vXThAkT7M99fX3tf6elpalTp04KDQ3VTz/9pKNHj6pXr17y8PDQf/7znwIdDwAANyPyNQAAeefSonv58uUOz+fPn6+QkBBt2bJFLVu2tE/39fVVaGholn1899132r17t1atWqVSpUqpQYMGmjhxol588UWNGzdOnp6elo4BAICbHfkaAIC8K1TndCcmJkqSSpQo4TD9ww8/VFBQkOrUqaNRo0YpOTnZPm/jxo2qW7euSpUqZZ8WGRmpM2fOaNeuXQUTOAAAtxDyNQAAOefSX7qvlJ6eriFDhuiOO+5QnTp17NO7d++u8PBwhYWFafv27XrxxRcVGxurJUuWSJLi4+MdErgk+/P4+Pgsl5WSkqKUlBT78zNnzjh7OAAA3JTI1wAA5E6hKboHDBignTt36scff3SY/tRTT9n/rlu3rkqXLq02bdrowIEDqly5cp6WNXnyZI0fPz5f8QIAcCsiXwMAkDuF4vDygQMH6uuvv9batWtVtmzZa7Zt2rSpJGn//v2SpNDQUB07dsyhTcbz7M4rGzVqlBITE+2PP//8M79DAADgpke+BgAg91xadBtjNHDgQH3++edas2aNKlaseN3XbNu2TZJUunRpSVJERIR27Nih48eP29usXLlSAQEBqlWrVpZ9eHl5KSAgwOEBAACyRr4GACDvXHp4+YABA7Rw4UJ98cUXKlq0qP2crsDAQPn4+OjAgQNauHChOnbsqJIlS2r79u16/vnn1bJlS9WrV0+S1K5dO9WqVUuPP/64pkyZovj4eI0ePVoDBgyQl5eXK4cHAMBNgXwNAEDeufSX7tmzZysxMVGtW7dW6dKl7Y+PP/5YkuTp6alVq1apXbt2qlGjhoYNG6auXbvqq6++svfh7u6ur7/+Wu7u7oqIiFDPnj3Vq1cvh/uEAgCAvCNfAwCQdy79pdsYc8355cqV0/r166/bT3h4uJYtW+assAAAwBXI1wAA5F2huJAaAAAAAAA3I4puAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsUsTVAaBgxcTEOKWfoKAglS9f3il9AQCAzMjZAHBzoOi+RZxNOCabm5t69uzplP58fH21JyaGJA4AgJMdS0qSm83mtJzt6+OjmD17yNkA4CIU3beI82fPyKSn65FJsxVSsWq++jp+cJ8+Gd1fCQkJJHAAAJws8cIFpRujd7p0UbWgoHz1tTchQU8tWULOBgAXoui+xYRUrKoyNeu7OgwAAHAd1YKC1CAszNVhAADyiQupAQAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABYp4uoAbkZxcXFKSEjIdz8xMTFOiAYAAGTFWflaImcDALJH0e1kcXFxqlGzps4nJ7s6FAAAkI24uDjVrFFDyefPuzoUAMBNjqLbyRISEnQ+OVmPTJqtkIpV89VX7IbVWjlrspMiAwAAGRISEpR8/rze6dJF1YKC8t3fyn379O+1a50QGQDgZkPRbZGQilVVpmb9fPVx/OA+J0UDAACyUi0oSA3CwvLdz14nHaYOALj5cCE1AAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFKLoBAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWMSlRffkyZPVuHFjFS1aVCEhIercubNiY2Md2ly4cEEDBgxQyZIl5e/vr65du+rYsWMObeLi4tSpUyf5+voqJCREI0aMUGpqakEOBQCAmxb5GgCAvHNp0b1+/XoNGDBAP//8s1auXKlLly6pXbt2OnfunL3N888/r6+++kqLFy/W+vXrdeTIEXXp0sU+Py0tTZ06ddLFixf1008/acGCBZo/f77GjBnjiiEBAHDTIV8DAJB3RVy58OXLlzs8nz9/vkJCQrRlyxa1bNlSiYmJmjt3rhYuXKi7775bkhQdHa2aNWvq559/VrNmzfTdd99p9+7dWrVqlUqVKqUGDRpo4sSJevHFFzVu3Dh5enq6YmgAANw0yNcAAORdoTqnOzExUZJUokQJSdKWLVt06dIltW3b1t6mRo0aKl++vDZu3ChJ2rhxo+rWratSpUrZ20RGRurMmTPatWtXAUYPAMCtgXwNAEDOufSX7iulp6dryJAhuuOOO1SnTh1JUnx8vDw9PVWsWDGHtqVKlVJ8fLy9zZUJPGN+xryspKSkKCUlxf78zJkzzhoGAAA3NfI1AAC5U2h+6R4wYIB27typRYsWWb6syZMnKzAw0P4oV66c5csEAOBmQL4GACB3CkXRPXDgQH399ddau3atypYta58eGhqqixcv6vTp0w7tjx07ptDQUHubq6+OmvE8o83VRo0apcTERPvjzz//dOJoAAC4OZGvAQDIPZcW3cYYDRw4UJ9//rnWrFmjihUrOsxv2LChPDw8tHr1avu02NhYxcXFKSIiQpIUERGhHTt26Pjx4/Y2K1euVEBAgGrVqpXlcr28vBQQEODwAAAAWSNfAwCQdy49p3vAgAFauHChvvjiCxUtWtR+TldgYKB8fHwUGBiovn37aujQoSpRooQCAgI0aNAgRUREqFmzZpKkdu3aqVatWnr88cc1ZcoUxcfHa/To0RowYIC8vLxcOTwAAG4K5GsAAPLOpUX37NmzJUmtW7d2mB4dHa3evXtLkl5//XW5ubmpa9euSklJUWRkpGbNmmVv6+7urq+//lr9+/dXRESE/Pz8FBUVpQkTJhTUMAAAuKmRrwEAyDuXFt3GmOu28fb21syZMzVz5sxs24SHh2vZsmXODA0AAPx/5GsAAPKuUFxIDQAAAACAmxFFNwAAAAAAFqHoBgAAAADAIhTdAAAAAABYhKIbAAAAAACLUHQDAAAAAGARim4AAAAAACxC0Q0AAAAAgEUougEAAAAAsAhFNwAAAAAAFqHoBgAAAADAIhTdAAAAAABYhKIbAAAAAACL5Kno/uOPP5wdBwAAsAA5GwAA18pT0V2lShXddddd+u9//6sLFy44OyYAAOAk5GwAAFwrT0X3b7/9pnr16mno0KEKDQ3V008/rV9++cXZsQEAgHwiZwMA4Fp5KrobNGigN954Q0eOHNG8efN09OhR3XnnnapTp46mTZumf/75x9lxAgCAPCBnAwDgWvm6kFqRIkXUpUsXLV68WK+88or279+v4cOHq1y5curVq5eOHj3qrDgBAEA+kLMBAHCNfBXdv/76q5599lmVLl1a06ZN0/Dhw3XgwAGtXLlSR44c0QMPPOCsOAEAQD6QswEAcI0ieXnRtGnTFB0drdjYWHXs2FHvv/++OnbsKDe3yzV8xYoVNX/+fFWoUMGZsQIAgFwiZwMA4Fp5Krpnz56tJ554Qr1791bp0qWzbBMSEqK5c+fmKzgAAJA/5GwAAFwrT0X3vn37rtvG09NTUVFReekeAAA4CTkbAADXytM53dHR0Vq8eHGm6YsXL9aCBQvyHRQAAHAOcjYAAK6Vp6J78uTJCgoKyjQ9JCRE//nPf/IdFAAAcA5yNgAArpWnojsuLk4VK1bMND08PFxxcXH5DgoAADgHORsAANfKU9EdEhKi7du3Z5r++++/q2TJkvkOCgAAOAc5GwAA18pT0f3YY49p8ODBWrt2rdLS0pSWlqY1a9boueeeU7du3ZwdIwAAyCNyNgAArpWnq5dPnDhRhw4dUps2bVSkyOUu0tPT1atXL84PAwCgECFnAwDgWnkquj09PfXxxx9r4sSJ+v333+Xj46O6desqPDzc2fGhEIuJiXFaX0FBQSpfvrzT+gMAXEbOhuS8nE2+BoDcy1PRnaFatWqqVq2as2LBDeJswjHZ3NzUs2dPp/Xp4+urPTExJHIAsAg5+9Z0LClJbjab03K2r4+PYvbsIV8DQC7kqehOS0vT/PnztXr1ah0/flzp6ekO89esWeOU4FA4nT97RiY9XY9Mmq2QilXz3d/xg/v0yej+SkhIIIkDgJORs29tiRcuKN0YvdOli6plceu43NibkKCnliwhXwNALuWp6H7uuec0f/58derUSXXq1JHNZnN2XLgBhFSsqjI167s6DADANZCzIUnVgoLUICzM1WEAwC0pT0X3okWL9Mknn6hjx47OjgcAADgRORsAANfK0y3DPD09VaVKFWfHAgAAnIycDQCAa+Wp6B42bJjeeOMNGWOcHQ8AAHAicjYAAK6Vp8PLf/zxR61du1bffvutateuLQ8PD4f5S5YscUpwAAAgf8jZAAC4Vp6K7mLFiunBBx90diwAAMDJyNkAALhWnoru6OhoZ8cBAAAsQM4GAMC18nROtySlpqZq1apVevvtt3X27FlJ0pEjR5SUlOS04AAAQP6RswEAcJ08/dJ9+PBhtW/fXnFxcUpJSdE999yjokWL6pVXXlFKSormzJnj7DgBAEAekLMBAHCtPP3S/dxzz6lRo0Y6deqUfHx87NMffPBBrV692mnBAQCA/CFnAwDgWnn6pfuHH37QTz/9JE9PT4fpFSpU0N9//+2UwAAAQP6RswEAcK08/dKdnp6utLS0TNP/+usvFS1aNN9BAQAA5yBnAwDgWnkqutu1a6fp06fbn9tsNiUlJWns2LHq2LGjs2IDAAD5RM4GAMC18nR4+WuvvabIyEjVqlVLFy5cUPfu3bVv3z4FBQXpo48+cnaMAAAgj8jZAAC4Vp6K7rJly+r333/XokWLtH37diUlJalv377q0aOHw0VaAACAa5GzAQBwrTwV3ZJUpEgR9ezZ05mxAAAAC5CzAQBwnTwV3e+///415/fq1StPwQAAAOciZwMA4Fp5Krqfe+45h+eXLl1ScnKyPD095evrSwIHAKCQIGcDAOBaebp6+alTpxweSUlJio2N1Z133slFWQAAKETI2QAAuFaeiu6sVK1aVS+//HKmb9QBAEDhQs4GAKDgOK3oli5fqOXIkSPO7BIAAFiAnA0AQMHI0zndX375pcNzY4yOHj2qt956S3fccYdTAgMAAPlHzgYAwLXy9Et3586dHR5dunTRuHHjVK9ePc2bNy/H/Xz//fe67777FBYWJpvNpqVLlzrM7927t2w2m8Ojffv2Dm1OnjypHj16KCAgQMWKFVPfvn2VlJSUl2EBAHDTIWcDAOBaefqlOz093SkLP3funOrXr68nnnhCXbp0ybJN+/btFR0dbX/u5eXlML9Hjx46evSoVq5cqUuXLqlPnz566qmntHDhQqfECADAjYycDQCAa+Wp6HaWDh06qEOHDtds4+XlpdDQ0CznxcTEaPny5dq8ebMaNWokSZoxY4Y6duyoqVOnKiwszOkxAwBwKyJnAwCQN3kquocOHZrjttOmTcvLIuzWrVunkJAQFS9eXHfffbcmTZqkkiVLSpI2btyoYsWK2ZO3JLVt21Zubm7atGmTHnzwwSz7TElJUUpKiv35mTNn8hUjAACF1Y2cs8nXAICbQZ6K7q1bt2rr1q26dOmSqlevLknau3ev3N3ddfvtt9vb2Wy2fAXXvn17denSRRUrVtSBAwf00ksvqUOHDtq4caPc3d0VHx+vkJAQh9cUKVJEJUqUUHx8fLb9Tp48WePHj89XbAAA3Ahu5JxNvgYA3AzyVHTfd999Klq0qBYsWKDixYtLkk6dOqU+ffqoRYsWGjZsmFOC69atm/3vunXrql69eqpcubLWrVunNm3a5LnfUaNGOXzzf+bMGZUrVy5fsQIAUBjdyDmbfA0AuBnk6erlr732miZPnmxP3pJUvHhxTZo0Sa+99prTgrtapUqVFBQUpP3790uSQkNDdfz4cYc2qampOnnyZLbnlEmXzzkLCAhweAAAcDO6kXM2+RoAcDPIU9F95swZ/fPPP5mm//PPPzp79my+g8rOX3/9pRMnTqh06dKSpIiICJ0+fVpbtmyxt1mzZo3S09PVtGlTy+IAAOBGQc4GAMC18nR4+YMPPqg+ffrotddeU5MmTSRJmzZt0ogRI7K9jUhWkpKS7N+AS9LBgwe1bds2lShRQiVKlND48ePVtWtXhYaG6sCBA3rhhRdUpUoVRUZGSpJq1qyp9u3bq1+/fpozZ44uXbqkgQMHqlu3blwFFQAAkbMBAHC1PBXdc+bM0fDhw9W9e3ddunTpckdFiqhv37569dVXc9zPr7/+qrvuusv+POO8raioKM2ePVvbt2/XggULdPr0aYWFhaldu3aaOHGiw30/P/zwQw0cOFBt2rSRm5ubunbtqjfffDMvwwIA4KZDzgYAwLXyVHT7+vpq1qxZevXVV3XgwAFJUuXKleXn55erflq3bi1jTLbzV6xYcd0+SpQooYULF+ZquQAA3CrI2QAAuFaezunOcPToUR09elRVq1aVn5/fNZMxAABwHXI2AACukaei+8SJE2rTpo2qVaumjh076ujRo5Kkvn37Ou3WIwAAIP/I2QAAuFaeiu7nn39eHh4eiouLk6+vr336o48+quXLlzstOAAAkD/kbAAAXCtP53R/9913WrFihcqWLeswvWrVqjp8+LBTAgMAAPlHzgYAwLXy9Ev3uXPnHL4tz3Dy5EmHq5QCAADXImcDAOBaeSq6W7Rooffff9/+3GazKT09XVOmTHG4nQgAAHAtcjYAAK6Vp8PLp0yZojZt2ujXX3/VxYsX9cILL2jXrl06efKkNmzY4OwYAQBAHpGzAQBwrTz90l2nTh3t3btXd955px544AGdO3dOXbp00datW1W5cmVnxwgAAPKInA0AgGvl+pfuS5cuqX379pozZ47+9a9/WRETAABwAnI2AACul+tfuj08PLR9+3YrYgEAAE5EzgYAwPXydHh5z549NXfuXGfHAgAAnIycDQCAa+XpQmqpqamaN2+eVq1apYYNG8rPz89h/rRp05wSHAAAyB9yNgAArpWrovuPP/5QhQoVtHPnTt1+++2SpL179zq0sdlszosOAADkCTkbAIDCIVdFd9WqVXX06FGtXbtWkvToo4/qzTffVKlSpSwJDgAA5A05GwCAwiFX53QbYxyef/vttzp37pxTAwIAAPlHzgYAoHDI04XUMlyd0AEAQOFEzgYAwDVyVXTbbLZM539xPhgAAIUPORsAgMIhV+d0G2PUu3dveXl5SZIuXLigZ555JtOVUJcsWeK8CAEAQK6RswEAKBxyVXRHRUU5PO/Zs6dTgwEAAM5BzgYAoHDIVdEdHR1tVRwAAMCJyNkAABQO+bqQGgAAAAAAyB5FNwAAAAAAFqHoBgAAAADAIhTdAAAAAABYhKIbAAAAAACLUHQDAAAAAGARim4AAAAAACxC0Q0AAAAAgEUougEAAAAAsAhFNwAAAAAAFqHoBgAAAADAIhTdAAAAAABYhKIbAAAAAACLUHQDAAAAAGARim4AAAAAACxC0Q0AAAAAgEUougEAAAAAsAhFNwAAAAAAFqHoBgAAAADAIhTdAAAAAABYhKIbAAAAAACLUHQDAAAAAGARim4AAAAAACxC0Q0AAAAAgEUougEAAAAAsAhFNwAAAAAAFqHoBgAAAADAIhTdAAAAAABYhKIbAAAAAACLUHQDAAAAAGARim4AAAAAACxC0Q0AAAAAgEUougEAAAAAsIhLi+7vv/9e9913n8LCwmSz2bR06VKH+cYYjRkzRqVLl5aPj4/atm2rffv2ObQ5efKkevTooYCAABUrVkx9+/ZVUlJSAY4CAICbHzkbAIC8cWnRfe7cOdWvX18zZ87Mcv6UKVP05ptvas6cOdq0aZP8/PwUGRmpCxcu2Nv06NFDu3bt0sqVK/X111/r+++/11NPPVVQQwAA4JZAzgYAIG+KuHLhHTp0UIcOHbKcZ4zR9OnTNXr0aD3wwAOSpPfff1+lSpXS0qVL1a1bN8XExGj58uXavHmzGjVqJEmaMWOGOnbsqKlTpyosLKzAxgIAwM2MnA0AQN4U2nO6Dx48qPj4eLVt29Y+LTAwUE2bNtXGjRslSRs3blSxYsXsyVuS2rZtKzc3N23atCnbvlNSUnTmzBmHBwAAyBurcjb5GgBwMyi0RXd8fLwkqVSpUg7TS5UqZZ8XHx+vkJAQh/lFihRRiRIl7G2yMnnyZAUGBtof5cqVc3L0AADcOqzK2eRrAMDNoNAW3VYaNWqUEhMT7Y8///zT1SEBAICrkK8BADeDQlt0h4aGSpKOHTvmMP3YsWP2eaGhoTp+/LjD/NTUVJ08edLeJiteXl4KCAhweAAAgLyxKmeTrwEAN4NCW3RXrFhRoaGhWr16tX3amTNntGnTJkVEREiSIiIidPr0aW3ZssXeZs2aNUpPT1fTpk0LPGYAAG5F5GwAALLn0quXJyUlaf/+/fbnBw8e1LZt21SiRAmVL19eQ4YM0aRJk1S1alVVrFhR//d//6ewsDB17txZklSzZk21b99e/fr105w5c3Tp0iUNHDhQ3bp14yqoAAA4ETkbAIC8cWnR/euvv+quu+6yPx86dKgkKSoqSvPnz9cLL7ygc+fO6amnntLp06d15513avny5fL29ra/5sMPP9TAgQPVpk0bubm5qWvXrnrzzTcLfCwAANzMyNkAAOSNS4vu1q1byxiT7XybzaYJEyZowoQJ2bYpUaKEFi5caEV4AADg/yNnAwCQN4X2nG4AAAAAAG50FN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARYq4OgAgQ0xMjFP6CQoKUvny5Z3SFwAAcES+BoDcoeiGy51NOCabm5t69uzplP58fH21JyaGRA4AgBMdS0qSm83mtHzt6+OjmD17yNcAbnoU3XC582fPyKSn65FJsxVSsWq++jp+cJ8+Gd1fCQkJJHEAAJwo8cIFpRujd7p0UbWgoHz1tTchQU8tWUK+BnBLoOhGoRFSsarK1Kzv6jAAAMA1VAsKUoOwMFeHAQA3DC6kBgAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwSKEuuseNGyebzebwqFGjhn3+hQsXNGDAAJUsWVL+/v7q2rWrjh075sKIAQC4NZGzAQDIWqEuuiWpdu3aOnr0qP3x448/2uc9//zz+uqrr7R48WKtX79eR44cUZcuXVwYLQAAty5yNgAAmRVxdQDXU6RIEYWGhmaanpiYqLlz52rhwoW6++67JUnR0dGqWbOmfv75ZzVr1qygQwUA4JZGzgYAILNC/0v3vn37FBYWpkqVKqlHjx6Ki4uTJG3ZskWXLl1S27Zt7W1r1Kih8uXLa+PGja4KFwCAWxY5GwCAzAr1L91NmzbV/PnzVb16dR09elTjx49XixYttHPnTsXHx8vT01PFihVzeE2pUqUUHx9/zX5TUlKUkpJif37mzBkrwgcA4JZhRc4mXwMAbgaFuuju0KGD/e969eqpadOmCg8P1yeffCIfH5889zt58mSNHz/eGSECAABZk7PJ1wCAm0GhP7z8SsWKFVO1atW0f/9+hYaG6uLFizp9+rRDm2PHjmV5PtmVRo0apcTERPvjzz//tDBqAABuPc7I2eRrAMDN4IYqupOSknTgwAGVLl1aDRs2lIeHh1avXm2fHxsbq7i4OEVERFyzHy8vLwUEBDg8AACA8zgjZ5OvAQA3g0J9ePnw4cN13333KTw8XEeOHNHYsWPl7u6uxx57TIGBgerbt6+GDh2qEiVKKCAgQIMGDVJERARXQQUAoICRswEAyFqhLrr/+usvPfbYYzpx4oSCg4N155136ueff1ZwcLAk6fXXX5ebm5u6du2qlJQURUZGatasWS6OGgCAWw85GwCArBXqonvRokXXnO/t7a2ZM2dq5syZBRQRAADICjkbAICs3VDndAMAAAAAcCOh6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFCvV9uoG8iomJcUo/QUFBKl++vFP6AgAAjsjXAG4FFN24qZxNOCabm5t69uzplP58fH21JyaGRA4AgBMdS0qSm83mtHzt6+OjmD17yNcACiWKbtxUzp89I5OerkcmzVZIxar56uv4wX36ZHR/JSQkkMQBAHCixAsXlG6M3unSRdWCgvLV196EBD21ZAn5GkChRdGNm1JIxaoqU7O+q8MAAADXUC0oSA3CwlwdBgBYigupAQAAAABgEYpuAAAAAAAswuHlwHVwZVUAAAo/Z+VriZwNwLkouoFscCV0AAAKP2dfCV3iaugAnIuiG8gGV0IHAKDwc+aV0CWuhg7A+Si6gevgSugAABR+XAkdQGHFhdQAAAAAALAIRTcAAAAAABah6AYAAAAAwCIU3QAAAAAAWISiGwAAAAAAi1B0AwAAAABgEYpuAAAAAAAsQtENAAAAAIBFirg6AOBWEhMT45R+goKCVL58eaf0BQAAMiNnA3AWim6gAJxNOCabm5t69uzplP58fH21JyaGJA4AgJMdS0qSm83mtJzt6+OjmD17yNnALYyiGygA58+ekUlP1yOTZiukYtV89XX84D59Mrq/EhISSOAAADhZ4oULSjdG73TpompBQfnqa29Cgp5asoScDdziKLqBAhRSsarK1Kzv6jAAAMB1VAsKUoOwMFeHAeAmwIXUAAAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiRVwdAABkJy4uTgkJCU7pKygoSOXLl3dKXwAA4H/I18C1UXQDKJTi4uJUo2ZNnU9Odkp/Pr6+2hMTQyIHAMCJ4uLiVLNGDSWfP++U/nx9fBSzZw/5GjcVim4AhVJCQoLOJyfrkUmzFVKxar76On5wnz4Z3V8JCQkkcQAAnCghIUHJ58/rnS5dVC0oKF997U1I0FNLlpCvcdOh6AZQqIVUrKoyNeu7OgwAAHAN1YKC1CAszNVhAIUSF1IDAAAAAMAiFN0AAAAAAFiEohsAAAAAAItwTjdwi3PmbT4kKSUlRV5eXvnuJyYmxgnRAABw83BmziZfAwWHohu4QTkjyR09elQPPfywLjjpNh+SZHNzk0lPd1p/AADc6JyVsx9+6CGdv3DBCRFJbjab0o1xSl8Aro2iG7jBnE04Jpubm3r27Om0Pp1xWy5Jit2wWitnTXZKfxl9AQBwozqWlCQ3m82pOdsZt+ZauW+f/r12rVP7ApA9im7gBnP+7BmZ9HSnFrbOui3X8YP7JDnnNl8ZfQEAcKNKvHBB6cY4tbh1xq259v7/Q9Sd2ReA7FF0AzcoClsAAG4MFLfArY2iGwBwQ3HmhYSCgoJUvnx5p/QFAAD+x9kX672Rc/ZNU3TPnDlTr776quLj41W/fn3NmDFDTZo0cXVYAAAniouLU42aNXU+Odkp/fn4+mpPTMwNm8RvVORsALi5xcXFqWaNGkp24sV6fX18FLNnzw2Zs2+Kovvjjz/W0KFDNWfOHDVt2lTTp09XZGSkYmNjFRIS4urwABQSzrqtibNus+Lsvm7kb4BzKiEhQeeTk51yTYPjB/fpk9H99cMPP6hmzZr5ju1WWP/OQM4GcD3OvA0ZOds1EhISlHz+vFOuZyBdPr3iqSVLbticfVMU3dOmTVO/fv3Up08fSdKcOXP0zTffaN68eRo5cqSLowPgas6+4rszb4vmzL4K86+2zjrELOODmDOuaeDs/aIwr//ChJwNIDtWXO3dmbdGc2ZfhfVXW2fna2dcz0By/r5R0Ov/hi+6L168qC1btmjUqFH2aW5ubmrbtq02btzowsgAFBZWXPG9sPWV8attQkJCoUzgzjwk3FmcuV8U5vVfmJCzAVyLM6/2LllzazRn9JXxq21hyxlWHBLuLM7cN1yx/m/4ojshIUFpaWkqVaqUw/RSpUppz549Wb4mJSVFKSkp9ueJiYmSpDNnzuQ7nqSkJEnS3zHbdTH5XL76+ufQvpu+r8IcG33dPLFl9HXpwvl895V6MaVQ9nXpwuUEuWXLFvv7UH65ubkp3Qm/wsfGxup8crJa9BqgYqFl8tXXX7u2aes3nxS6/SJj/SclJTkll2T0YZz0i0phkducXRD5+vejR3Xu4sV897f3n3+c1h99ua6vwhzbrdTX+UuXnLItU1JTndafM/s6f+mSJOflbGfm6+Tz5zWoeXOVDQjIV1+/HTmij7dvd/r/pTPXvzNydo7ztbnB/f3330aS+emnnxymjxgxwjRp0iTL14wdO9ZI4sGDBw8ePAr1488//yyIVFpgcpuzydc8ePDgweNGeFwvX9/wv3QHBQXJ3d1dx44dc5h+7NgxhYaGZvmaUaNGaejQofbn6enpOnnypEqWLCmbzXbdZZ45c0blypXTn3/+qYB8fgtUmDCuG8fNOCbp5hzXzTgmiXFZzRijs2fPKswJ58EVJrnN2fnN19dSWLZ1XhG/axG/axG/axH//+Q0X9/wRbenp6caNmyo1atXq3PnzpIuJ+XVq1dr4MCBWb7Gy8sr05UHixUrlutlBwQE3JA72vUwrhvHzTgm6eYc1804JolxWSkwMNCly7dCbnO2s/L1tRSGbZ0fxO9axO9axO9axH9ZTvL1DV90S9LQoUMVFRWlRo0aqUmTJpo+fbrOnTtnvzIqAAAoHMjZAIBbzU1RdD/66KP6559/NGbMGMXHx6tBgwZavnx5pgu1AAAA1yJnAwBuNTdF0S1JAwcOzPZwcmfz8vLS2LFjMx3ydqNjXDeOm3FM0s05rptxTBLjQv4UZM7Ozo2+rYnftYjftYjftYg/92zG3GT3IwEAAAAAoJBwc3UAAAAAAADcrCi6AQAAAACwCEU3AAAAAAAWoejOg5kzZ6pChQry9vZW06ZN9csvv7g6pGyNGzdONpvN4VGjRg37/AsXLmjAgAEqWbKk/P391bVrVx07dsyhj7i4OHXq1Em+vr4KCQnRiBEjlJqaWqDj+P7773XfffcpLCxMNptNS5cudZhvjNGYMWNUunRp+fj4qG3bttq3b59Dm5MnT6pHjx4KCAhQsWLF1LdvXyUlJTm02b59u1q0aCFvb2+VK1dOU6ZMcdmYevfunWnbtW/fvlCPafLkyWrcuLGKFi2qkJAQde7cWbGxsQ5tnLXPrVu3Trfffru8vLxUpUoVzZ8/36Xjat26dabt9cwzzxTqcc2ePVv16tWz36cyIiJC3377rX3+jbitrjemG3E7Ie/+/e9/q3nz5vL19c3x/b2dlU+cIbfLOXToUKb9O+OxePFie7us5i9atMjl8UvO+x8t6NhPnjypQYMGqXr16vLx8VH58uU1ePBgJSYmOrSzct3n9vPp4sWLVaNGDXl7e6tu3bpatmyZw/yc/C84U27if/fdd9WiRQsVL15cxYsXV9u2bTO1z8nnKFfFP3/+/EyxeXt7O7QpyPWfm9iz+h+12Wzq1KmTvU1BrvvrfZ7OSk5yuNPrPYNcWbRokfH09DTz5s0zu3btMv369TPFihUzx44dc3VoWRo7dqypXbu2OXr0qP3xzz//2Oc/88wzply5cmb16tXm119/Nc2aNTPNmze3z09NTTV16tQxbdu2NVu3bjXLli0zQUFBZtSoUQU6jmXLlpl//etfZsmSJUaS+fzzzx3mv/zyyyYwMNAsXbrU/P777+b+++83FStWNOfPn7e3ad++valfv775+eefzQ8//GCqVKliHnvsMfv8xMREU6pUKdOjRw+zc+dO89FHHxkfHx/z9ttvu2RMUVFRpn379g7b7uTJkw5tCtuYIiMjTXR0tNm5c6fZtm2b6dixoylfvrxJSkqyt3HGPvfHH38YX19fM3ToULN7924zY8YM4+7ubpYvX+6ycbVq1cr069fPYXslJiYW6nF9+eWX5ptvvjF79+41sbGx5qWXXjIeHh5m586dxpgbc1tdb0w34nZC3o0ZM8ZMmzbNDB061AQGBuboNc7IJ86S2+WkpqY67NtHjx4148ePN/7+/ubs2bP2dpJMdHS0Q7srx+eq+I1xzv+oK2LfsWOH6dKli/nyyy/N/v37zerVq03VqlVN165dHdpZte5z+/l0w4YNxt3d3UyZMsXs3r3bjB492nh4eJgdO3bY2+Tkf8FZcht/9+7dzcyZM83WrVtNTEyM6d27twkMDDR//fWXvU1OPke5Kv7o6GgTEBDgEFt8fLxDm4Ja/7mN/cSJEw5x79y507i7u5vo6Gh7m4Jc99f7PH21nORwK+o9iu5catKkiRkwYID9eVpamgkLCzOTJ092YVTZGzt2rKlfv36W806fPm08PDzM4sWL7dNiYmKMJLNx40ZjzOUd2c3NzeGNYPbs2SYgIMCkpKRYGnt2rv6HSk9PN6GhoebVV1+1Tzt9+rTx8vIyH330kTHGmN27dxtJZvPmzfY23377rbHZbObvv/82xhgza9YsU7x4cYdxvfjii6Z69eoWjyjzmIy5/Ib1wAMPZPuawj4mY4w5fvy4kWTWr19vjHHePvfCCy+Y2rVrOyzr0UcfNZGRkVYPyRiTeVzGXP6g+Nxzz2X7mhthXMYYU7x4cfPee+/dNNvKmP+NyZibZzshd6Kjo3NUdDsrnziDs5bToEED88QTTzhMy8kH0/zKa/zO+B91VexX++STT4ynp6e5dOmSfZpV6z63n08feeQR06lTJ4dpTZs2NU8//bQxJmf/C66M/2qpqammaNGiZsGCBfZp1/sc5Uy5jf9670kFuf7zu+5ff/11U7RoUYcfIgpy3V8pJ/9fOcnhVtR7HF6eCxcvXtSWLVvUtm1b+zQ3Nze1bdtWGzdudGFk17Zv3z6FhYWpUqVK6tGjh+Li4iRJW7Zs0aVLlxzGU6NGDZUvX94+no0bN6pu3boqVaqUvU1kZKTOnDmjXbt2FexAsnHw4EHFx8c7jCMwMFBNmzZ1GEexYsXUqFEje5u2bdvKzc1NmzZtsrdp2bKlPD097W0iIyMVGxurU6dOFdBoHK1bt04hISGqXr26+vfvrxMnTtjn3QhjyjisrkSJEpKct89t3LjRoY+MNgX1f3j1uDJ8+OGHCgoKUp06dTRq1CglJyfb5xX2caWlpWnRokU6d+6cIiIiboptdfWYMtzI2wnWclY+cQZnLGfLli3atm2b+vbtm2negAEDFBQUpCZNmmjevHkyTr6DbH7iz+//qCtjv1JiYqICAgJUpEgRh+nOXvd5+Xx6vfexnPwvOIszPl8nJyfr0qVLmfLytT5HOUte409KSlJ4eLjKlSunBx54wGH/Laj174x1P3fuXHXr1k1+fn4O0wti3efF9fZ9q+q9ItdvggwJCQlKS0tzeKOXpFKlSmnPnj0uiuramjZtqvnz56t69eo6evSoxo8frxYtWmjnzp2Kj4+Xp6dnpvPcSpUqpfj4eElSfHx8luPNmFcYZMSRVZxXjiMkJMRhfpEiRVSiRAmHNhUrVszUR8a84sWLWxJ/dtq3b68uXbqoYsWKOnDggF566SV16NBBGzdulLu7e6EfU3p6uoYMGaI77rhDderUsS/TGftcdm3OnDmj8+fPy8fHx4ohScp6XJLUvXt3hYeHKywsTNu3b9eLL76o2NhYLVmypFCPa8eOHYqIiNCFCxfk7++vzz//XLVq1dK2bdtu2G2V3ZikG3c7oWA4K584K5b8Lmfu3LmqWbOmmjdv7jB9woQJuvvuu+Xr66vvvvtOzz77rJKSkjR48GCXx++M/1FXxX6lhIQETZw4UU899ZTDdCvWfV4+n2a3Hq/czzOmZdfGWZzx+frFF19UWFiYQ6F0vc9Rroy/evXqmjdvnurVq6fExERNnTpVzZs3165du1S2bNkCW//5Xfe//PKLdu7cqblz5zpML6h1nxfXy+GnTp2ypN6j6L7JdejQwf53vXr11LRpU4WHh+uTTz7hg2Eh161bN/vfdevWVb169VS5cmWtW7dObdq0cWFkOTNgwADt3LlTP/74o6tDcarsxnXlB6u6deuqdOnSatOmjQ4cOKDKlSsXdJg5Vr16dW3btk2JiYn69NNPFRUVpfXr17s6rHzJbky1atW6YbcT/mfkyJF65ZVXrtkmJibG4aKhhUlO48+v8+fPa+HChfq///u/TPOunHbbbbfp3LlzevXVV3NU+Fkdv5X/owW17s+cOaNOnTqpVq1aGjdunMO8/Kx7ZO3ll1/WokWLtG7dOoeLkRXmz1EREREOR2A1b95cNWvW1Ntvv62JEye6MLLcmTt3rurWrasmTZo4TC/M695VKLpzISgoSO7u7pmu3nvs2DGFhoa6KKrcKVasmKpVq6b9+/frnnvu0cWLF3X69GmHX7OuHE9oaGimq/VljL+wjDkjjmPHjql06dL26ceOHVODBg3sbY4fP+7wutTUVJ08edJhrFlt2yuX4UqVKlVSUFCQ9u/frzZt2hTqMQ0cOFBff/21vv/+e5UtW9Y+PTQ01Cn7XHbjCggIsPTLpOzGlZWmTZtKkvbv36/KlSsX2nF5enqqSpUqkqSGDRtq8+bNeuONN/Too4/esNsquzG9/fbbmdreKNsJ/zNs2DD17t37mm0qVaqUp76dlU+uJafx53c5n376qZKTk9WrV6/rtm3atKkmTpyolJQUeXl5FYr4r4xNyt3/qCtjP3v2rNq3b6+iRYvq888/l4eHxzXb52bdZycvn0+zex+78n0uY1p2/wvOkp/P11OnTtXLL7+sVatWqV69etdse/XnKGdxRn3g4eGh2267Tfv375dUcOs/P7GfO3dOixYt0oQJE667HKvWfV5cL4e7u7tbUu9xTncueHp6qmHDhlq9erV9Wnp6ulavXu3wbVVhlpSUpAMHDqh06dJq2LChPDw8HMYTGxuruLg4+3giIiK0Y8cOh+SzcuVKBQQE2A/XdLWKFSsqNDTUYRxnzpzRpk2bHMZx+vRpbdmyxd5mzZo1Sk9Ptyf0iIgIff/997p06ZK9zcqVK1W9evUCP7Q8K3/99ZdOnDhhf/MtjGMyxmjgwIH6/PPPtWbNmkyHtjtrn4uIiHDoI6ONVf+H1xtXVrZt2yZJDtursI0rK+np6UpJSblht1VWMsaUlRt1O93KgoODVaNGjWs+rryORW44K584I/78Lmfu3Lm6//77FRwcfN2227ZtU/HixXNU9BVU/FfGJuXuf9RVsZ85c0bt2rWTp6envvzyy0y3gMpufDld99nJy+fT672P5eR/wVny+vl6ypQpmjhxopYvX+5w/n12rv4c5SzOqA/S0tK0Y8cOe2wFtf7zE/vixYuVkpKinj17Xnc5Vq37vLjevm9ZvZfnS7DdohYtWmS8vLzM/Pnzze7du81TTz1lihUrluky/4XFsGHDzLp168zBgwfNhg0bTNu2bU1QUJA5fvy4MebyLYHKly9v1qxZY3799VcTERFhIiIi7K/PuDVHu3btzLZt28zy5ctNcHBwgd8y7OzZs2br1q1m69atRpKZNm2a2bp1qzl8+LAx5vJtFYoVK2a++OILs337dvPAAw9keYuX2267zWzatMn8+OOPpmrVqg63/zh9+rQpVaqUefzxx83OnTvNokWLjK+vr2W317rWmM6ePWuGDx9uNm7caA4ePGhWrVplbr/9dlO1alVz4cKFQjum/v37m8DAQLNu3TqH20QkJyfb2zhjn8u43cOIESNMTEyMmTlzpqW3bLreuPbv328mTJhgfv31V3Pw4EHzxRdfmEqVKpmWLVsW6nGNHDnSrF+/3hw8eNBs377djBw50thsNvPdd98ZY27MbXWtMd2o2wl5d/jwYbN161b7bbMy3nOvvH1W9erVzZIlS+zPnZFPnOV6y/nrr79M9erVzaZNmxxet2/fPmOz2cy3336bqc8vv/zSvPvuu2bHjh1m3759ZtasWcbX19eMGTPG5fE763/UFbEnJiaapk2bmrp165r9+/c75IrU1FRjjLXr/nqfTx9//HEzcuRIe/sNGzaYIkWKmKlTp5qYmBgzduzYLG8Zdr3/BWfJbfwvv/yy8fT0NJ9++qnDus74387p5yhXxT9+/HizYsUKc+DAAbNlyxbTrVs34+3tbXbt2uUwxoJY/7mNPcOdd95pHn300UzTC3rdX69GGDlypHn88cft7XOSw62o9yi682DGjBmmfPnyxtPT0zRp0sT8/PPPrg4pW48++qgpXbq08fT0NGXKlDGPPvqo2b9/v33++fPnzbPPPmuKFy9ufH19zYMPPmiOHj3q0MehQ4dMhw4djI+PjwkKCjLDhg1zuP1FQVi7dq2RlOkRFRVljLl8a4X/+7//M6VKlTJeXl6mTZs2JjY21qGPEydOmMcee8z4+/ubgIAA06dPH4cPXsYY8/vvv5s777zTeHl5mTJlypiXX37ZJWNKTk427dq1M8HBwcbDw8OEh4ebfv36ZfpnL2xjymo8+v/3JM3grH1u7dq1pkGDBsbT09NUqlTJYRkFPa64uDjTsmVLU6JECePl5WWqVKliRowY4XBv2cI4rieeeMKEh4cbT09PExwcbNq0aWMvuI25MbfVtcZ0o24n5F1UVFSW/7tr1661t7n6PcpZ+cQZrrecgwcPZhqPMcaMGjXKlCtXzqSlpWXq89tvvzUNGjQw/v7+xs/Pz9SvX9/MmTMny7YFHb8z/0cLOvbscrokc/DgQWOM9ev+Wp9PW7VqZf/MlOGTTz4x1apVM56enqZ27drmm2++cZifk/8FZ8pN/OHh4Vmu67FjxxpjTI4/R7kq/iFDhtjblipVynTs2NH89ttvDv0V5PrP7b6zZ88eI8nhM0OGgl7316sRoqKiTKtWrTK95no53Nn1ns0YJ98jAgAAAAAASOKcbgAAAAAALEPRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbjjV/PnzVaxYsVy9pnfv3urcubMl8RQW48aNU4MGDZzWX17Ws1UOHTokm82mbdu2WbocZ6/DgpacnKyuXbsqICBANptNp0+fdnVIAG5x5OyskbPzj5wNOKLoRo5kl2TXrVvn8Gb06KOPau/evQUb3HVcHaPVbDabli5d6jBt+PDhWr16dYEsH4XTggUL9MMPP+inn37S0aNHFRgY6OqQANykyNk5R85GVsjZcLYirg4ANxcfHx/5+Pi4OoxCx9/fX/7+/q4OAy504MAB1axZU3Xq1Mm2zcWLF+Xp6VmAUQG4lZGzs0bOBjkbzsYv3XCqrA6hmjRpkkJCQlS0aFE9+eSTGjlyZJaHHE2dOlWlS5dWyZIlNWDAAF26dMk+LyUlRcOHD1eZMmXk5+enpk2bat26dfb5hw8f1n333afixYvLz89PtWvX1rJly3To0CHdddddkqTixYvLZrOpd+/e2cb/2WefqXbt2vLy8lKFChX02muvOcyvUKGCJk6cqMcee0x+fn4qU6aMZs6c6TBfkh588EHZbDb786wOs5o3b559WaVLl9bAgQPt86ZNm6a6devKz89P5cqV07PPPqukpKRs475axuFjn3zyiVq0aCEfHx81btxYe/fu1ebNm9WoUSP5+/urQ4cO+ueff+yvS09P14QJE1S2bFl5eXmpQYMGWr58uUPfv/zyi2677TZ5e3urUaNG2rp1a6bl79y5Ux06dJC/v79KlSqlxx9/XAkJCdnGm7HfLF26VFWrVpW3t7ciIyP1559/Zmr7wQcfqEKFCgoMDFS3bt109uxZ+7yUlBQNHjxYISEh8vb21p133qnNmzfb52f8grJ69Wo1atRIvr6+at68uWJjYx2WMXv2bFWuXFmenp6qXr26PvjgA4f5NptN7733nh588EH5+vqqatWq+vLLL7MdX+vWrfXaa6/p+++/l81mU+vWrSX9b3/q1auXAgIC9NRTT0mSfvzxR/t2K1eunAYPHqxz587Z+zt+/Ljuu+8++fj4qGLFivrwww9VoUIFTZ8+XVLWhw+ePn1aNpvN4f/metupdevWGjx4sF544QWVKFFCoaGhGjdunMPYTp8+raefflqlSpWSt7e36tSpo6+//lrnzp1TQECAPv30U4f2S5culZ+fn8N2A+Aa5OwKksjZ5GxH5Oz/IWc7kQFyICoqyjzwwAOZpq9du9ZIMqdOnTLGGBMdHW0CAwPt8//73/8ab29vM2/ePBMbG2vGjx9vAgICTP369R36DggIMM8884yJiYkxX331lfH19TXvvPOOvc2TTz5pmjdvbr7//nuzf/9+8+qrrxovLy+zd+9eY4wxnTp1Mvfcc4/Zvn27OXDggPnqq6/M+vXrTWpqqvnss8+MJBMbG2uOHj1qTp8+neUYf/31V+Pm5mYmTJhgYmNjTXR0tPHx8THR0dH2NuHh4aZo0aJm8uTJJjY21rz55pvG3d3dfPfdd8YYY44fP24kmejoaHP06FFz/PhxY4wxY8eOdRjzrFmzjLe3t5k+fbqJjY01v/zyi3n99dft819//XWzZs0ac/DgQbN69WpTvXp1079/f/v8q9fz1Q4ePGgkmRo1apjly5eb3bt3m2bNmpmGDRua1q1bmx9//NH89ttvpkqVKuaZZ56xv27atGkmICDAfPTRR2bPnj3mhRdeMB4eHvb1fPbsWRMcHGy6d+9udu7cab766itTqVIlI8ls3brVGGPMqVOnTHBwsBk1apSJiYkxv/32m7nnnnvMXXfdlW280dHRxsPDwzRq1Mj89NNP5tdffzVNmjQxzZs3t7cZO3as8ff3N126dDE7duww33//vQkNDTUvvfSSvc3gwYNNWFiYWbZsmdm1a5eJiooyxYsXNydOnDDG/G9/bdq0qVm3bp3ZtWuXadGihcNylixZYjw8PMzMmTNNbGysee2114y7u7tZs2aNvY0kU7ZsWbNw4UKzb98+M3jwYOPv729fztVOnDhh+vXrZyIiIszRo0ft7cLDw01AQICZOnWq2b9/v/3h5+dnXn/9dbN3716zYcMGc9ttt5nevXvb++vQoYOpX7++2bhxo/n1119N8+bNjY+Pj30fytj+GdskY7tIMmvXrs3xdmrVqpUJCAgw48aNM3v37jULFiwwNpvNvr+npaWZZs2amdq1a5vvvvvO/r+3bNkyY4wx/fr1Mx07dnRYF/fff7/p1atXtvsCgPwjZ19GziZnG0POJmcXDhTdyJGoqCjj7u5u/Pz8HB7e3t7XTOBNmzY1AwYMcOjrjjvuyJTAw8PDTWpqqn3aww8/bB599FFjjDGHDx827u7u5u+//3bop02bNmbUqFHGGGPq1q1rxo0bl2XsV3/IyE737t3NPffc4zBtxIgRplatWvbn4eHhpn379g5tHn30UdOhQwf7c0nm888/d2hzdQIPCwsz//rXv64Zz5UWL15sSpYsaX+e0wT+3nvv2ad99NFHRpJZvXq1fdrkyZNN9erVHeL697//7dBX48aNzbPPPmuMMebtt982JUuWNOfPn7fPnz17tkOymDhxomnXrp1DH3/++af9Q1RWoqOjjSTz888/26fFxMQYSWbTpk3GmMvr0NfX15w5c8beZsSIEaZp06bGGGOSkpKMh4eH+fDDD+3zL168aMLCwsyUKVOMMf/bF1atWmVv88033xhJ9jE1b97c9OvXzyG+hx9+2CERSTKjR4+2P09KSjKSzLfffpvl+Iwx5rnnnjOtWrVymBYeHm46d+7sMK1v377mqaeecpj2ww8/GDc3N3P+/HkTGxtrJJlffvkl07rKTQLPyXZq1aqVufPOOx3aNG7c2Lz44ovGGGNWrFhh3Nzcst2umzZtMu7u7ubIkSPGGGOOHTtmihQpYtatW5fNWgLgDOTsy8jZ5GxjyNnk7MKBw8uRY3fddZe2bdvm8Hjvvfeu+ZrY2Fg1adLEYdrVzyWpdu3acnd3tz8vXbq0jh8/LknasWOH0tLSVK1aNft5Vv7+/lq/fr0OHDggSRo8eLAmTZqkO+64Q2PHjtX27dtzPb6YmBjdcccdDtPuuOMO7du3T2lpafZpERERDm0iIiIUExOT4+UcP35cR44cUZs2bbJts2rVKrVp00ZlypRR0aJF9fjjj+vEiRNKTk7O8XIkqV69eva/S5UqJUmqW7euw7SM9XzmzBkdOXIky3WQMb6YmBjVq1dP3t7e9vlXr4/ff/9da9euddhWNWrUkCT79spKkSJF1LhxY/vzGjVqqFixYg7rtkKFCipatKj9+ZX7yYEDB3Tp0iWH+D08PNSkSZNM2+fK9VK6dGlJsveT3X5wrT78/PwUEBBg7yM3GjVq5PD8999/1/z58x3WX2RkpNLT03Xw4EHFxMSoSJEiatiwof01GesqN3K6na4cp+S4zrdt26ayZcuqWrVqWS6jSZMmql27thYsWCBJ+u9//6vw8HC1bNkyV7ECyD1y9mXkbHL21X2Qs8nZrsCF1JBjfn5+qlKlisO0v/76yyl9e3h4ODy32WxKT0+XJCUlJcnd3V1btmxxSPKS7Bc6efLJJxUZGalvvvlG3333nSZPnqzXXntNgwYNckp8znS9i9YcOnRI9957r/r3769///vfKlGihH788Uf17dtXFy9elK+vb46XdeV6tdlsWU7LWM/OkpSUpPvuu0+vvPJKpnkZyTKvrrWf5LWfjPWS236cFYufn5/D86SkJD399NMaPHhwprbly5fP0ZWG3dwuf59qjLFPu/J8y4zl5GQ7XWucObkA05NPPqmZM2dq5MiRio6OVp8+fezrHIB1yNnOQc7OO3I2ORv/wy/dsFT16tUdLoohKdPz67ntttuUlpam48ePq0qVKg6P0NBQe7ty5crpmWee0ZIlSzRs2DC9++67kmS/suSV33xnpWbNmtqwYYPDtA0bNqhatWoOHxx+/vlnhzY///yzatasaX/u4eFxzWUVLVpUFSpUyPZ2JFu2bFF6erpee+01NWvWTNWqVdORI0euGbszBAQEKCwsLMt1UKtWLUmX19H27dt14cIF+/yr18ftt9+uXbt2qUKFCpm219XJ6kqpqan69ddf7c9jY2N1+vRph3V7LRkXUbky/kuXLmnz5s32+HMiu/0gN33kx+23367du3dnWndVqlSRp6enatSoodTUVG3ZssX+mox1lSE4OFiSdPToUfu0q+/JmtftdKV69erpr7/+uuaHip49e+rw4cN68803tXv3bkVFReWobwAFj5ydGTk7a+Tsy8jZyCmKblhq0KBBmjt3rhYsWKB9+/Zp0qRJ2r59e66+NatWrZp69OihXr16acmSJTp48KB++eUXTZ48Wd98840kaciQIVqxYoUOHjyo3377TWvXrrW/8YeHh8tms+nrr7/WP//8k+0VRYcNG6bVq1dr4sSJ2rt3rxYsWKC33npLw4cPd2i3YcMGTZkyRXv37tXMmTO1ePFiPffcc/b5Gck5Pj5ep06dynJZ48aN02uvvaY333xT+/bt02+//aYZM2ZIkqpUqaJLly5pxowZ+uOPP/TBBx9ozpw5OV5f+TFixAi98sor+vjjjxUbG6uRI0dq27Zt9vF1795dNptN/fr10+7du7Vs2TJNnTrVoY8BAwbo5MmTeuyxx7R582YdOHBAK1asUJ8+fa75wcbDw0ODBg3Spk2btGXLFvXu3VvNmjXL8tDGrPj5+al///4aMWKEli9frt27d6tfv35KTk5W3759c7UO5s+fr9mzZ2vfvn2aNm2alixZkmk/sMqLL76on376SQMHDtS2bdu0b98+ffHFF/Yr5VavXl3t27fX008/bV9XTz75pMM32D4+PmrWrJlefvllxcTEaP369Ro9erTDcvK6na7UqlUrtWzZUl27dtXKlSt18OBBffvttw5Xzy1evLi6dOmiESNGqF27dipbtqwT1hIAK5Czydnk7NwhZyOnKLphqR49emjUqFEaPny4br/9dh08eFC9e/d2OL8oJ6Kjo9WrVy8NGzZM1atXV+fOnbV582aVL19e0uVvxAcMGKCaNWuqffv2qlatmmbNmiVJKlOmjMaPH6+RI0eqVKlSDrf5uNLtt9+uTz75RIsWLVKdOnU0ZswYTZgwIdPtSoYNG6Zff/1Vt912myZNmqRp06YpMjLSPv+1117TypUrVa5cOd12221ZLisqKkrTp0/XrFmzVLt2bd17773at2+fJKl+/fqaNm2aXnnlFdWpU0cffvihJk+enKv1lVeDBw/W0KFDNWzYMNWtW1fLly/Xl19+qapVq0q6fGjgV199pR07dui2227Tv/71r0yHOmV8856WlqZ27dqpbt26GjJkiIoVK2Y/hCorvr6+evHFF9W9e3fdcccd8vf318cff5yr+F9++WV17dpVjz/+uG6//Xbt379fK1asUPHixXPcR+fOnfXGG29o6tSpql27tt5++21FR0fbbxlitXr16mn9+vXau3evWrRoodtuu01jxoxRWFiYvU10dLTCwsLUqlUrdenSRU899ZRCQkIc+pk3b55SU1PVsGFDDRkyRJMmTXKYn9ftdLXPPvtMjRs31mOPPaZatWrphRdeyPQBIOMwyyeeeCIPawRAQSFnk7PJ2blDzkZO2cyVJxAABeCee+5RaGhopvso3ggqVKigIUOGaMiQIa4O5aYyf/58DRkyxOFwK+ROYd43P/jgAz3//PM6cuSI/dBRADcGcjauRs7Ov8K8b5KzrcGF1GCp5ORkzZkzR5GRkXJ3d9dHH32kVatWaeXKla4ODYDFkpOTdfToUb388st6+umnSd5AIUfOBm5d5GxrcXg5LGWz2bRs2TK1bNlSDRs21FdffaXPPvtMbdu2dXVoACw2ZcoU1ahRQ6GhoRo1apSrwwFwHeRs4NZFzrYWh5cDAAAAAGARfukGAAAAAMAiFN0AAAAAAFiEohsAAAAAAItQdAMAAAAAYBGKbgAAAAAALELRDQAAAACARSi6AQAAAACwCEU3AAAAAAAWoegGAAAAAMAi/w/P3Fj1OiM2ggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create the figure and axes for the subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5)) # 1 row, 2 columns of subplots.\n",
        "\n",
        "# Plot the first histogram\n",
        "axes[0].hist(df_train['target'], bins=20, color='skyblue', edgecolor='black')\n",
        "axes[0].set_title('Not Standardized and Normalized')\n",
        "axes[0].set_xlabel('Highest optical mode phonon frequency')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# Plot the second histogram\n",
        "axes[1].hist(norm_train, bins=20, color='lightcoral', edgecolor='black')\n",
        "axes[1].set_title('Standardized and Normalized')\n",
        "axes[1].set_xlabel('Highest optical mode phonon frequency')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH5DVsB_Hdzs"
      },
      "outputs": [],
      "source": [
        "df_train['target'] = norm_train\n",
        "df_test['target'] = norm_test\n",
        "test = df_test.to_dict(orient = 'records')\n",
        "train = df_train.to_dict(orient = 'records')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataset for training\n",
        "We select 200 random datapoints for illustration"
      ],
      "metadata": {
        "id": "ibXLKVrY9z95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHgU8S3egHGm",
        "outputId": "faa295f9-2005-4d00-98bf-1515557ec934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[107, 548, 178, 834, 545, 220, 78, 776, 1098, 680, 697, 106, 326, 276, 690, 683, 502, 335, 3, 893, 179, 773, 143, 13, 646, 918, 208, 89, 189, 291, 258, 43, 597, 881, 976, 543, 961, 75, 624, 703, 1068, 989, 423, 1079, 645, 25, 815, 1050, 889, 1102, 995, 1066, 859, 767, 1044, 65, 371, 169, 997, 536, 348, 685, 1135, 801, 150, 935, 793, 688, 33, 387, 180, 744, 26, 720, 469, 830, 1089, 1026, 122, 589, 1013, 477, 1114, 990, 363, 314, 910, 775, 944, 90, 20, 275, 934, 395, 525, 556, 860, 0, 222, 128, 1045, 727, 181, 887, 17, 1097, 1119, 827, 59, 660, 537, 967, 67, 848, 713, 48, 858, 131, 294, 762, 481, 160, 117, 1062, 1100, 627, 299, 596, 797, 754, 259, 1095, 691, 1059, 244, 341, 791, 885, 127, 278, 931, 42, 715, 921, 923, 622, 878, 57, 324, 665, 1058, 1054, 281, 1024, 214, 1117, 755, 460, 696, 190, 1036, 498, 416, 659, 541, 88, 1025, 164, 403, 1034, 408, 35, 397, 549, 472, 174, 649, 171, 698, 1101, 273, 1094, 277, 991, 1008, 526, 810, 459, 12, 1012, 250, 738, 943, 741, 861, 734, 443, 919, 702, 394]\n",
            "No of randomly selected datapoints 200\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def Rand(start, end, num):\n",
        "  random.seed(123)\n",
        "  random_ids =  random.sample(range(start, end),num)\n",
        "  return random_ids\n",
        "random_pts = Rand(0, 1137, 200)\n",
        "print(Rand(0, 1137, 200))\n",
        "\n",
        "train_200 = []\n",
        "for i in sorted(random_pts, reverse=True):\n",
        "    train_200.append(train[i])\n",
        "\n",
        "print(f'No of randomly selected datapoints',len(train_200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgymAAizEMfU"
      },
      "outputs": [],
      "source": [
        "from jarvis.core.atoms import Atoms\n",
        "import os\n",
        "output_dir = \"./phonons/data\"\n",
        "os.makedirs(output_dir, exist_ok=True) # create data dir if does not exist\n",
        "output_file_name=\"id_prop.csv\"\n",
        "with open(output_file_name, 'w') as f:\n",
        "    for i in train_200:\n",
        "        atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "        jid = i[\"jid\"]\n",
        "        poscar_name = os.path.join(output_dir, \"POSCAR-\" + str(jid) + \".vasp\") # path to poscar\n",
        "        target = i['target']\n",
        "        atoms.write_poscar(poscar_name)\n",
        "        f.write(\"%s,%6f\\n\" % (poscar_name, target))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a configuration file\n",
        "\n",
        "Save a configuration file contains all parameters required for defining the model architecture and training process. We will use the alignn_atomwise model for making graph level prediction. Note that alignn_atomwise can make both graph and atom level predictions.\n",
        "\n",
        " Key parameters are described below:\n",
        "\n",
        " ``random_seed:`` Establishes a seed for random number generation to ensure reproducibility.\n",
        "   \n",
        "   ``classification_threshold:``  If classifying, sets a decision boundary to classify between classes; is null, indicating regression.\n",
        "\n",
        "   `n_val: `                 Explicitly defines the number of data points to be used for validation, and is null, meaning using a ratio instead.\n",
        "   \n",
        "   `n_test: `                Explicitly defines the number of data points to be used for testing, and is null, meaning using a ratio instead.\n",
        "   \n",
        "   `n_train:`                Explicitly defines the number of data points to be used for training, and is null, meaning using a ratio instead.\n",
        "   \n",
        "   `train_ratio: `          Specifies the proportion of the dataset to be used for training the model.\n",
        "   \n",
        "   `val_ratio:`             Specifies the proportion of the dataset to be used for validation.\n",
        "   \n",
        "   `test_ratio:`          Specifies the proportion of the dataset to be used for testing the model.\n",
        "\n",
        "   `epochs:`                 Defines the number of complete passes through the training dataset.\n",
        "\n",
        "   `batch_size:`             Specifies the number of samples to process in one iteration during training.\n",
        "\n",
        "   `weight_decay:`          Applies regularization to prevent overfitting by penalizing large weights.\n",
        "\n",
        "   `learning_rate:`         Controls how much the model weights are adjusted during training.\n",
        "\n",
        "   `model.name:`            Specifies the name of the model architecture, in this case 'alignn_atomwise'.\n",
        "\n",
        "   `model.alignn_layers:`    Defines the number of ALIGNN layers in the model.\n",
        "   \n",
        "   `model.gcn_layers:`       Defines the number of GCN layers in the model.\n",
        "   \n",
        "   `model.atom_input_features:`   Specifies the number of features representing each atom before processing.\n",
        "   \n",
        "   `model.edge_input_features:`   Specifies the number of features representing each edge before processing.\n",
        "   \n",
        "   `model.triplet_input_features:` Specifies the number of features representing triplets of atoms before processing.\n",
        "   \n",
        "   `model.embedding_features:`  Specifies the dimension of the embedding layer used in the model.\n",
        "   \n",
        "   `model.hidden_features:`   Specifies the number of hidden units in the neural network layers.\n",
        "   \n",
        "   `model.output_features:`   Specifies the dimension of the final output of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "F-UesAwg-Ej2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zzZPXb0EWMt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "config = {\n",
        "    \"version\": \"112bbedebdaecf59fb18e11c929080fb2f358246\",\n",
        "    \"dataset\": \"user_data\",\n",
        "    \"target\": \"target\",\n",
        "    \"atom_features\": \"cgcnn\",\n",
        "    \"neighbor_strategy\": \"k-nearest\",\n",
        "    \"id_tag\": \"jid\",\n",
        "    \"dtype\": \"float32\",\n",
        "    \"random_seed\": 123,\n",
        "    \"classification_threshold\": None,\n",
        "    \"n_val\": None,\n",
        "    \"n_test\": None,\n",
        "    \"n_train\": None,\n",
        "    \"train_ratio\": 0.8,\n",
        "    \"val_ratio\": 0.2,\n",
        "    \"test_ratio\": 0,\n",
        "    \"target_multiplication_factor\": None,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 2,\n",
        "    \"weight_decay\": 1e-05,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"filename\": \"A\",\n",
        "    \"warmup_steps\": 2000,\n",
        "    \"criterion\": \"mse\",\n",
        "    \"optimizer\": \"adamw\",\n",
        "    \"scheduler\": \"onecycle\",\n",
        "    \"pin_memory\": False,\n",
        "    \"save_dataloader\": False,\n",
        "    \"write_checkpoint\": True,\n",
        "    \"write_predictions\": True,\n",
        "    \"store_outputs\": True,\n",
        "    \"progress\": True,\n",
        "    \"log_tensorboard\": False,\n",
        "    \"standard_scalar_and_pca\": False,\n",
        "    \"use_canonize\": True,\n",
        "    \"num_workers\": 0,\n",
        "    \"cutoff\": 8.0,\n",
        "    \"cutoff_extra\": 3.0,\n",
        "    \"max_neighbors\": 12,\n",
        "    \"keep_data_order\": True,\n",
        "    \"normalize_graph_level_loss\": False,\n",
        "    \"distributed\": False,\n",
        "    \"data_parallel\": False,\n",
        "    \"n_early_stopping\": None,\n",
        "    \"output_dir\": \"temp\",\n",
        "    \"use_lmdb\": True,\n",
        "    \"model\": {\n",
        "        \"name\": \"alignn_atomwise\",\n",
        "        \"alignn_layers\": 4,\n",
        "        \"gcn_layers\": 4,\n",
        "        \"atom_input_features\": 92,\n",
        "        \"edge_input_features\": 80,\n",
        "        \"triplet_input_features\": 40,\n",
        "        \"embedding_features\": 64,\n",
        "        \"hidden_features\": 64,\n",
        "        \"output_features\": 1,\n",
        "        \"grad_multiplier\": -1,\n",
        "        \"calculate_gradient\": False,\n",
        "        \"atomwise_output_features\": 0,\n",
        "        \"graphwise_weight\": 1.0,\n",
        "        \"gradwise_weight\": 1.0,\n",
        "        \"stresswise_weight\": 0.0,\n",
        "        \"atomwise_weight\": 0.0,\n",
        "        \"link\": \"identity\",\n",
        "        \"zero_inflated\": False,\n",
        "        \"classification\": False,\n",
        "        \"force_mult_natoms\": False,\n",
        "        \"energy_mult_natoms\": False,\n",
        "        \"include_pos_deriv\": False,\n",
        "        \"use_cutoff_function\": False,\n",
        "        \"inner_cutoff\": 3.0,\n",
        "        \"stress_multiplier\": 1.0,\n",
        "        \"add_reverse_forces\": True,\n",
        "        \"lg_on_fly\": True,\n",
        "        \"batch_stress\": True,\n",
        "        \"multiply_cutoff\": False,\n",
        "        \"use_penalty\": True,\n",
        "        \"extra_features\": 0,\n",
        "        \"exponent\": 5,\n",
        "        \"penalty_factor\": 0.1,\n",
        "        \"penalty_threshold\": 1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "file_path = \"config.json\"\n",
        "\n",
        "with open(file_path, \"w\") as json_file:\n",
        "    json.dump(config, json_file)\n",
        "with open(\"/content/form_energy/config.json\", \"w\") as json_file:\n",
        "    json.dump(config, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a model from scratch"
      ],
      "metadata": {
        "id": "uGm6I61w_Vf4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0qt6Ix6ewkn"
      },
      "source": [
        "We now construct a model from scratch for predicting the highest optical phonon mode frequncy of the materials.\n",
        "The config.json file contains all the required parameters for load and training the alignn_atomwise model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1o15GibqPyF"
      },
      "source": [
        "Training the SCRATCH model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbodu7rBEMtY",
        "outputId": "7b59495c-b6ce-4d06-f751-23b6b13994aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "/usr/local/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "world_size 0\n",
            "root_dir ./\n",
            "id_prop_csv_file exists True\n",
            "len dataset 200\n",
            "train_stress False\n",
            "Using LMDB dataset.\n",
            "MAX val: 0.400152\n",
            "MIN val: -0.998882\n",
            "MAD: 0.17105815215\n",
            "Data error Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
            "data range 0.400152 -0.998882\n",
            "line_graph True\n",
            "100% 160/160 [00:02<00:00, 61.31it/s]\n",
            "data range -0.418111 -0.9782\n",
            "line_graph True\n",
            "100% 40/40 [00:00<00:00, 67.03it/s]\n",
            "n_train: 160\n",
            "n_val  : 40\n",
            "n_test : 0\n",
            "rank 0\n",
            "world_size 0\n",
            "config: {'version': '112bbedebdaecf59fb18e11c929080fb2f358246', 'dataset': 'user_data', 'target': 'target', 'atom_features': 'cgcnn', 'neighbor_strategy': 'k-nearest', 'id_tag': 'jid', 'dtype': 'float32', 'random_seed': 123, 'classification_threshold': None, 'n_val': None, 'n_test': None, 'n_train': None, 'train_ratio': 0.8, 'val_ratio': 0.2, 'test_ratio': 0.0, 'target_multiplication_factor': None, 'epochs': 10, 'batch_size': 2, 'weight_decay': 1e-05, 'learning_rate': 0.001, 'filename': 'A', 'warmup_steps': 2000, 'criterion': 'mse', 'optimizer': 'adamw', 'scheduler': 'onecycle', 'pin_memory': False, 'save_dataloader': False, 'write_checkpoint': True, 'write_predictions': True, 'store_outputs': True, 'progress': True, 'log_tensorboard': False, 'standard_scalar_and_pca': False, 'use_canonize': True, 'num_workers': 0, 'cutoff': 8.0, 'cutoff_extra': 3.0, 'max_neighbors': 12, 'keep_data_order': True, 'normalize_graph_level_loss': False, 'distributed': False, 'data_parallel': False, 'n_early_stopping': None, 'output_dir': 'phonons', 'use_lmdb': True, 'model': {'name': 'alignn_atomwise', 'alignn_layers': 4, 'gcn_layers': 4, 'atom_input_features': 92, 'edge_input_features': 80, 'triplet_input_features': 40, 'embedding_features': 64, 'hidden_features': 64, 'output_features': 1, 'grad_multiplier': -1, 'calculate_gradient': False, 'atomwise_output_features': 0, 'graphwise_weight': 1.0, 'gradwise_weight': 1.0, 'stresswise_weight': 0.0, 'atomwise_weight': 0.0, 'link': 'identity', 'zero_inflated': False, 'classification': False, 'force_mult_natoms': False, 'energy_mult_natoms': False, 'include_pos_deriv': False, 'use_cutoff_function': False, 'inner_cutoff': 3.0, 'stress_multiplier': 1.0, 'add_reverse_forces': True, 'lg_on_fly': True, 'batch_stress': True, 'multiply_cutoff': False, 'use_penalty': True, 'extra_features': 0, 'exponent': 5, 'penalty_factor': 0.1, 'penalty_threshold': 1.0, 'additional_output_features': 0, 'additional_output_weight': 0.0}}\n",
            "{'atom_features': 'cgcnn',\n",
            " 'batch_size': 2,\n",
            " 'classification_threshold': None,\n",
            " 'criterion': 'mse',\n",
            " 'cutoff': 8.0,\n",
            " 'cutoff_extra': 3.0,\n",
            " 'data_parallel': False,\n",
            " 'dataset': 'user_data',\n",
            " 'distributed': False,\n",
            " 'dtype': 'float32',\n",
            " 'epochs': 10,\n",
            " 'filename': 'A',\n",
            " 'id_tag': 'jid',\n",
            " 'keep_data_order': True,\n",
            " 'learning_rate': 0.001,\n",
            " 'log_tensorboard': False,\n",
            " 'max_neighbors': 12,\n",
            " 'model': {'add_reverse_forces': True,\n",
            "           'additional_output_features': 0,\n",
            "           'additional_output_weight': 0.0,\n",
            "           'alignn_layers': 4,\n",
            "           'atom_input_features': 92,\n",
            "           'atomwise_output_features': 0,\n",
            "           'atomwise_weight': 0.0,\n",
            "           'batch_stress': True,\n",
            "           'calculate_gradient': False,\n",
            "           'classification': False,\n",
            "           'edge_input_features': 80,\n",
            "           'embedding_features': 64,\n",
            "           'energy_mult_natoms': False,\n",
            "           'exponent': 5,\n",
            "           'extra_features': 0,\n",
            "           'force_mult_natoms': False,\n",
            "           'gcn_layers': 4,\n",
            "           'grad_multiplier': -1,\n",
            "           'gradwise_weight': 1.0,\n",
            "           'graphwise_weight': 1.0,\n",
            "           'hidden_features': 64,\n",
            "           'include_pos_deriv': False,\n",
            "           'inner_cutoff': 3.0,\n",
            "           'lg_on_fly': True,\n",
            "           'link': 'identity',\n",
            "           'multiply_cutoff': False,\n",
            "           'name': 'alignn_atomwise',\n",
            "           'output_features': 1,\n",
            "           'penalty_factor': 0.1,\n",
            "           'penalty_threshold': 1.0,\n",
            "           'stress_multiplier': 1.0,\n",
            "           'stresswise_weight': 0.0,\n",
            "           'triplet_input_features': 40,\n",
            "           'use_cutoff_function': False,\n",
            "           'use_penalty': True,\n",
            "           'zero_inflated': False},\n",
            " 'n_early_stopping': None,\n",
            " 'n_test': None,\n",
            " 'n_train': None,\n",
            " 'n_val': None,\n",
            " 'neighbor_strategy': 'k-nearest',\n",
            " 'normalize_graph_level_loss': False,\n",
            " 'num_workers': 0,\n",
            " 'optimizer': 'adamw',\n",
            " 'output_dir': 'phonons',\n",
            " 'pin_memory': False,\n",
            " 'progress': True,\n",
            " 'random_seed': 123,\n",
            " 'save_dataloader': False,\n",
            " 'scheduler': 'onecycle',\n",
            " 'standard_scalar_and_pca': False,\n",
            " 'store_outputs': True,\n",
            " 'target': 'target',\n",
            " 'target_multiplication_factor': None,\n",
            " 'test_ratio': 0.0,\n",
            " 'train_ratio': 0.8,\n",
            " 'use_canonize': True,\n",
            " 'use_lmdb': True,\n",
            " 'val_ratio': 0.2,\n",
            " 'version': '112bbedebdaecf59fb18e11c929080fb2f358246',\n",
            " 'warmup_steps': 2000,\n",
            " 'weight_decay': 1e-05,\n",
            " 'write_checkpoint': True,\n",
            " 'write_predictions': True}\n",
            "\n",
            "    _    _     ___ ____ _   _ _   _\n",
            "   / \\  | |   |_ _/ ___| \\ | | \\ | |\n",
            "  / _ \\ | |    | | |  _|  \\| |  \\| |\n",
            " / ___ \\| |___ | | |_| | |\\  | |\\  |\n",
            "/_/   \\_\\_____|___\\____|_| \\_|_| \\_|\n",
            "\n",
            "Model parameters 275457\n",
            "CUDA available False\n",
            "CUDA device count 0\n",
            "/usr/local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             0        41.4337    41.4337    0.0000     0.0000     0.0000     0.0000     9.25      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             0        2.8529     2.8529     0.0000     0.0000     0.0000     0.0000     0.95       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             1        19.7272    19.7272    0.0000     0.0000     0.0000     0.0000     7.51      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             1        2.9935     2.9935     0.0000     0.0000     0.0000     0.0000     1.24                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             2        13.1196    13.1196    0.0000     0.0000     0.0000     0.0000     8.25      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             2        2.8527     2.8527     0.0000     0.0000     0.0000     0.0000     1.35       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             3        12.6498    12.6498    0.0000     0.0000     0.0000     0.0000     9.60      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             3        2.1186     2.1186     0.0000     0.0000     0.0000     0.0000     0.93       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             4        13.8145    13.8145    0.0000     0.0000     0.0000     0.0000     7.42      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             4        2.1269     2.1269     0.0000     0.0000     0.0000     0.0000     1.12                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             5        10.3586    10.3586    0.0000     0.0000     0.0000     0.0000     8.79      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             5        1.8970     1.8970     0.0000     0.0000     0.0000     0.0000     0.93       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             6        10.5904    10.5904    0.0000     0.0000     0.0000     0.0000     8.70      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             6        1.7433     1.7433     0.0000     0.0000     0.0000     0.0000     1.22       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             7        10.8305    10.8305    0.0000     0.0000     0.0000     0.0000     7.49      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             7        2.4015     2.4015     0.0000     0.0000     0.0000     0.0000     0.96                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             8        8.5869     8.5869     0.0000     0.0000     0.0000     0.0000     9.01      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             8        1.3113     1.3113     0.0000     0.0000     0.0000     0.0000     0.96       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             9        6.8233     6.8233     0.0000     0.0000     0.0000     0.0000     7.92      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             9        1.4409     1.4409     0.0000     0.0000     0.0000     0.0000     1.41                 \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/train_alignn.py\", line 458, in <module>\n",
            "    train_for_folder(\n",
            "  File \"/usr/local/bin/train_alignn.py\", line 413, in train_for_folder\n",
            "    train_dgl(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/alignn/train.py\", line 610, in train_dgl\n",
            "    for dats, jid in zip(test_loader, test_loader.dataset.ids):\n",
            "AttributeError: 'NoneType' object has no attribute 'dataset'\n",
            "CPU times: user 952 ms, sys: 134 ms, total: 1.09 s\n",
            "Wall time: 1min 51s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!train_alignn.py --root_dir \"./\" --config \"config.json\" --output_dir=phonons --epochs 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune a model using a model pre-trained on formation energy dataset"
      ],
      "metadata": {
        "id": "Z4SmGZ4Y_dW9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvLtBAySqUHv"
      },
      "source": [
        "Now we load a pre-trained model on formation energy and then fine-tune it on the phonons dataset.\n",
        "We will compare the predictions made by the fine-tuned model and the model constructed from scratch for the higest optical phonon mode predictions.\n",
        "\n",
        "Loading a pre-trained model on formation energy and fine-tuning on the phonon dataset. Note the pre-trained model checkpoint was obtained by training on 1000 formation energy datapoints for 500 epochs. The above code for constructing the scratch model with the respective data and `--epochs 500` can be used to generate the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2g8yRTQIwCf",
        "outputId": "69101b6b-565e-4fbb-b589-647990d96483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "world_size 0\n",
            "root_dir ./\n",
            "id_prop_csv_file exists True\n",
            "len dataset 200\n",
            "train_stress False\n",
            "Restarting the model training: /content/form_energy/current_model.pt\n",
            "Rest config name='alignn_atomwise' alignn_layers=4 gcn_layers=4 atom_input_features=92 edge_input_features=80 triplet_input_features=40 embedding_features=64 hidden_features=64 output_features=1 grad_multiplier=-1 calculate_gradient=False atomwise_output_features=0 graphwise_weight=1.0 gradwise_weight=1.0 stresswise_weight=0.0 atomwise_weight=0.0 link='identity' zero_inflated=False classification=False force_mult_natoms=False energy_mult_natoms=False include_pos_deriv=False use_cutoff_function=False inner_cutoff=3.0 stress_multiplier=1.0 add_reverse_forces=True lg_on_fly=True batch_stress=True multiply_cutoff=False use_penalty=True extra_features=0 exponent=5 penalty_factor=0.1 penalty_threshold=1.0 additional_output_features=0 additional_output_weight=0.0\n",
            "model ALIGNNAtomWise(\n",
            "  (atom_embedding): MLPLayer(\n",
            "    (layer): Sequential(\n",
            "      (0): Linear(in_features=92, out_features=64, bias=True)\n",
            "      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (2): SiLU()\n",
            "    )\n",
            "  )\n",
            "  (edge_embedding): Sequential(\n",
            "    (0): RBFExpansion()\n",
            "    (1): MLPLayer(\n",
            "      (layer): Sequential(\n",
            "        (0): Linear(in_features=80, out_features=64, bias=True)\n",
            "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (2): SiLU()\n",
            "      )\n",
            "    )\n",
            "    (2): MLPLayer(\n",
            "      (layer): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (2): SiLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (angle_embedding): Sequential(\n",
            "    (0): RBFExpansion()\n",
            "    (1): MLPLayer(\n",
            "      (layer): Sequential(\n",
            "        (0): Linear(in_features=40, out_features=64, bias=True)\n",
            "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (2): SiLU()\n",
            "      )\n",
            "    )\n",
            "    (2): MLPLayer(\n",
            "      (layer): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (2): SiLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (alignn_layers): ModuleList(\n",
            "    (0-3): 4 x ALIGNNConv(\n",
            "      (node_update): EdgeGatedGraphConv(\n",
            "        (src_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (dst_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (edge_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (bn_edges): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (src_update): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (dst_update): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (bn_nodes): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (edge_update): EdgeGatedGraphConv(\n",
            "        (src_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (dst_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (edge_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (bn_edges): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (src_update): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (dst_update): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (bn_nodes): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (gcn_layers): ModuleList(\n",
            "    (0-3): 4 x EdgeGatedGraphConv(\n",
            "      (src_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (dst_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (edge_gate): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (bn_edges): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (src_update): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (dst_update): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (bn_nodes): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (readout): AvgPooling()\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Using LMDB dataset.\n",
            "MAX val: 0.400152\n",
            "MIN val: -0.998882\n",
            "MAD: 0.17105815215\n",
            "Data error Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
            "data range 0.400152 -0.998882\n",
            "line_graph True\n",
            "100% 160/160 [00:00<00:00, 1129778.86it/s]\n",
            "Reading dataset Atrain_data\n",
            "data range -0.418111 -0.9782\n",
            "line_graph True\n",
            "100% 40/40 [00:00<00:00, 403298.46it/s]\n",
            "Reading dataset Aval_data\n",
            "n_train: 160\n",
            "n_val  : 40\n",
            "n_test : 0\n",
            "rank 0\n",
            "world_size 0\n",
            "config: {'version': '112bbedebdaecf59fb18e11c929080fb2f358246', 'dataset': 'user_data', 'target': 'target', 'atom_features': 'cgcnn', 'neighbor_strategy': 'k-nearest', 'id_tag': 'jid', 'dtype': 'float32', 'random_seed': 123, 'classification_threshold': None, 'n_val': None, 'n_test': None, 'n_train': None, 'train_ratio': 0.8, 'val_ratio': 0.2, 'test_ratio': 0.0, 'target_multiplication_factor': None, 'epochs': 10, 'batch_size': 2, 'weight_decay': 1e-05, 'learning_rate': 0.001, 'filename': 'A', 'warmup_steps': 2000, 'criterion': 'mse', 'optimizer': 'adamw', 'scheduler': 'onecycle', 'pin_memory': False, 'save_dataloader': False, 'write_checkpoint': True, 'write_predictions': True, 'store_outputs': True, 'progress': True, 'log_tensorboard': False, 'standard_scalar_and_pca': False, 'use_canonize': True, 'num_workers': 0, 'cutoff': 8.0, 'cutoff_extra': 3.0, 'max_neighbors': 12, 'keep_data_order': True, 'normalize_graph_level_loss': False, 'distributed': False, 'data_parallel': False, 'n_early_stopping': None, 'output_dir': 'fine-tune', 'use_lmdb': True, 'model': {'name': 'alignn_atomwise', 'alignn_layers': 4, 'gcn_layers': 4, 'atom_input_features': 92, 'edge_input_features': 80, 'triplet_input_features': 40, 'embedding_features': 64, 'hidden_features': 64, 'output_features': 1, 'grad_multiplier': -1, 'calculate_gradient': False, 'atomwise_output_features': 0, 'graphwise_weight': 1.0, 'gradwise_weight': 1.0, 'stresswise_weight': 0.0, 'atomwise_weight': 0.0, 'link': 'identity', 'zero_inflated': False, 'classification': False, 'force_mult_natoms': False, 'energy_mult_natoms': False, 'include_pos_deriv': False, 'use_cutoff_function': False, 'inner_cutoff': 3.0, 'stress_multiplier': 1.0, 'add_reverse_forces': True, 'lg_on_fly': True, 'batch_stress': True, 'multiply_cutoff': False, 'use_penalty': True, 'extra_features': 0, 'exponent': 5, 'penalty_factor': 0.1, 'penalty_threshold': 1.0, 'additional_output_features': 0, 'additional_output_weight': 0.0}}\n",
            "{'atom_features': 'cgcnn',\n",
            " 'batch_size': 2,\n",
            " 'classification_threshold': None,\n",
            " 'criterion': 'mse',\n",
            " 'cutoff': 8.0,\n",
            " 'cutoff_extra': 3.0,\n",
            " 'data_parallel': False,\n",
            " 'dataset': 'user_data',\n",
            " 'distributed': False,\n",
            " 'dtype': 'float32',\n",
            " 'epochs': 10,\n",
            " 'filename': 'A',\n",
            " 'id_tag': 'jid',\n",
            " 'keep_data_order': True,\n",
            " 'learning_rate': 0.001,\n",
            " 'log_tensorboard': False,\n",
            " 'max_neighbors': 12,\n",
            " 'model': {'add_reverse_forces': True,\n",
            "           'additional_output_features': 0,\n",
            "           'additional_output_weight': 0.0,\n",
            "           'alignn_layers': 4,\n",
            "           'atom_input_features': 92,\n",
            "           'atomwise_output_features': 0,\n",
            "           'atomwise_weight': 0.0,\n",
            "           'batch_stress': True,\n",
            "           'calculate_gradient': False,\n",
            "           'classification': False,\n",
            "           'edge_input_features': 80,\n",
            "           'embedding_features': 64,\n",
            "           'energy_mult_natoms': False,\n",
            "           'exponent': 5,\n",
            "           'extra_features': 0,\n",
            "           'force_mult_natoms': False,\n",
            "           'gcn_layers': 4,\n",
            "           'grad_multiplier': -1,\n",
            "           'gradwise_weight': 1.0,\n",
            "           'graphwise_weight': 1.0,\n",
            "           'hidden_features': 64,\n",
            "           'include_pos_deriv': False,\n",
            "           'inner_cutoff': 3.0,\n",
            "           'lg_on_fly': True,\n",
            "           'link': 'identity',\n",
            "           'multiply_cutoff': False,\n",
            "           'name': 'alignn_atomwise',\n",
            "           'output_features': 1,\n",
            "           'penalty_factor': 0.1,\n",
            "           'penalty_threshold': 1.0,\n",
            "           'stress_multiplier': 1.0,\n",
            "           'stresswise_weight': 0.0,\n",
            "           'triplet_input_features': 40,\n",
            "           'use_cutoff_function': False,\n",
            "           'use_penalty': True,\n",
            "           'zero_inflated': False},\n",
            " 'n_early_stopping': None,\n",
            " 'n_test': None,\n",
            " 'n_train': None,\n",
            " 'n_val': None,\n",
            " 'neighbor_strategy': 'k-nearest',\n",
            " 'normalize_graph_level_loss': False,\n",
            " 'num_workers': 0,\n",
            " 'optimizer': 'adamw',\n",
            " 'output_dir': 'fine-tune',\n",
            " 'pin_memory': False,\n",
            " 'progress': True,\n",
            " 'random_seed': 123,\n",
            " 'save_dataloader': False,\n",
            " 'scheduler': 'onecycle',\n",
            " 'standard_scalar_and_pca': False,\n",
            " 'store_outputs': True,\n",
            " 'target': 'target',\n",
            " 'target_multiplication_factor': None,\n",
            " 'test_ratio': 0.0,\n",
            " 'train_ratio': 0.8,\n",
            " 'use_canonize': True,\n",
            " 'use_lmdb': True,\n",
            " 'val_ratio': 0.2,\n",
            " 'version': '112bbedebdaecf59fb18e11c929080fb2f358246',\n",
            " 'warmup_steps': 2000,\n",
            " 'weight_decay': 1e-05,\n",
            " 'write_checkpoint': True,\n",
            " 'write_predictions': True}\n",
            "\n",
            "    _    _     ___ ____ _   _ _   _\n",
            "   / \\  | |   |_ _/ ___| \\ | | \\ | |\n",
            "  / _ \\ | |    | | |  _|  \\| |  \\| |\n",
            " / ___ \\| |___ | | |_| | |\\  | |\\  |\n",
            "/_/   \\_\\_____|___\\____|_| \\_|_| \\_|\n",
            "\n",
            "Model parameters 275457\n",
            "CUDA available False\n",
            "CUDA device count 0\n",
            "/usr/local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             0        17.7145    17.7145    0.0000     0.0000     0.0000     0.0000     9.35      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             0        1.9890     1.9890     0.0000     0.0000     0.0000     0.0000     0.91       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             1        9.4885     9.4885     0.0000     0.0000     0.0000     0.0000     8.07      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             1        1.5695     1.5695     0.0000     0.0000     0.0000     0.0000     1.25       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             2        8.2454     8.2454     0.0000     0.0000     0.0000     0.0000     7.64      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             2        3.2029     3.2029     0.0000     0.0000     0.0000     0.0000     0.90                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             3        8.7966     8.7966     0.0000     0.0000     0.0000     0.0000     8.75      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             3        1.6306     1.6306     0.0000     0.0000     0.0000     0.0000     0.91                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             4        12.0617    12.0617    0.0000     0.0000     0.0000     0.0000     7.21      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             4        4.7342     4.7342     0.0000     0.0000     0.0000     0.0000     1.16                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             5        9.1671     9.1671     0.0000     0.0000     0.0000     0.0000     8.45      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             5        3.7029     3.7029     0.0000     0.0000     0.0000     0.0000     0.93                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             6        9.5443     9.5443     0.0000     0.0000     0.0000     0.0000     8.40      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             6        2.5511     2.5511     0.0000     0.0000     0.0000     0.0000     1.25                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             7        7.5291     7.5291     0.0000     0.0000     0.0000     0.0000     7.21      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             7        1.6623     1.6623     0.0000     0.0000     0.0000     0.0000     0.93                 \n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             8        7.7818     7.7818     0.0000     0.0000     0.0000     0.0000     8.86      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             8        1.2550     1.2550     0.0000     0.0000     0.0000     0.0000     0.91       Saving model\n",
            "Train Loss:  Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             9        5.7616     5.7616     0.0000     0.0000     0.0000     0.0000     9.28      \n",
            "Val Loss:    Epoch    Total      Graph      Atom       Grad       Stress     Addn.      Time      \n",
            "             9        1.6187     1.6187     0.0000     0.0000     0.0000     0.0000     1.62                 \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/train_alignn.py\", line 458, in <module>\n",
            "    train_for_folder(\n",
            "  File \"/usr/local/bin/train_alignn.py\", line 413, in train_for_folder\n",
            "    train_dgl(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/alignn/train.py\", line 610, in train_dgl\n",
            "    for dats, jid in zip(test_loader, test_loader.dataset.ids):\n",
            "AttributeError: 'NoneType' object has no attribute 'dataset'\n",
            "CPU times: user 869 ms, sys: 115 ms, total: 984 ms\n",
            "Wall time: 1min 40s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!train_alignn.py --root_dir \"./\" --config \"config.json\" --output_dir=fine-tune --restart_model_path='/content/form_energy/current_model.pt' --epochs 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_fc8H-Ke_srU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the scratch and fine-tuned model"
      ],
      "metadata": {
        "id": "JDOK57lE_tRs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM_fR9_aQUmr"
      },
      "source": [
        "We will now test both the scratch and the fine-tuned model on the prediction accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOH7urxCIwE2",
        "outputId": "d08381a8-0c0e-4082-a031-1663f6b07c4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jid 1250 out_data -0.7176106572151184 target [-0.65567356]\n",
            "jid 1250 out_data -0.5940433144569397 target [-0.65567356]\n",
            "jid 1248 out_data -0.7022044658660889 target [-0.68529905]\n",
            "jid 1248 out_data -0.5239205360412598 target [-0.68529905]\n",
            "jid 1244 out_data -0.6555631756782532 target [-0.66797093]\n",
            "jid 1244 out_data -0.594676673412323 target [-0.66797093]\n",
            "jid 1242 out_data -1.0048166513442993 target [-0.88708776]\n",
            "jid 1242 out_data -0.8426024913787842 target [-0.88708776]\n",
            "jid 1238 out_data -0.49109792709350586 target [-0.00447177]\n",
            "jid 1238 out_data -0.4467701017856598 target [-0.00447177]\n",
            "jid 1230 out_data -0.8997505903244019 target [-0.87870319]\n",
            "jid 1230 out_data -0.8484123349189758 target [-0.87870319]\n",
            "jid 1211 out_data -1.0311400890350342 target [-0.92062605]\n",
            "jid 1211 out_data -0.7992666959762573 target [-0.92062605]\n",
            "jid 1208 out_data -0.7137799263000488 target [-0.67859139]\n",
            "jid 1208 out_data -0.6586570143699646 target [-0.67859139]\n",
            "jid 1205 out_data -0.48045265674591064 target [-0.19675797]\n",
            "jid 1205 out_data -0.4118865430355072 target [-0.19675797]\n",
            "jid 1188 out_data -0.9106937646865845 target [-0.91503633]\n",
            "jid 1188 out_data -0.7407522797584534 target [-0.91503633]\n",
            "jid 1165 out_data -1.0055760145187378 target [-0.90832868]\n",
            "jid 1165 out_data -0.8294975757598877 target [-0.90832868]\n",
            "jid 1164 out_data -0.9510214328765869 target [-0.86249301]\n",
            "jid 1164 out_data -0.8010097742080688 target [-0.86249301]\n",
            "jid 1149 out_data -0.9527120590209961 target [-0.86193404]\n",
            "jid 1149 out_data -0.8090320825576782 target [-0.86193404]\n",
            "jid 1146 out_data -0.7389848232269287 target [-0.68418111]\n",
            "jid 1146 out_data -0.6545035243034363 target [-0.68418111]\n",
            "jid 1129 out_data -0.9440462589263916 target [-0.8921185]\n",
            "jid 1129 out_data -0.7351444959640503 target [-0.8921185]\n",
            "jid 1122 out_data -0.9795682430267334 target [-0.89938513]\n",
            "jid 1122 out_data -0.7885135412216187 target [-0.89938513]\n",
            "jid 1096 out_data -1.01365065574646 target [-0.93292342]\n",
            "jid 1096 out_data -0.8382999897003174 target [-0.93292342]\n",
            "jid 1062 out_data -0.9823489189147949 target [-0.91671325]\n",
            "jid 1062 out_data -0.7871743440628052 target [-0.91671325]\n",
            "jid 1061 out_data -0.8123930096626282 target [-0.78591392]\n",
            "jid 1061 out_data -0.728181004524231 target [-0.78591392]\n",
            "jid 1058 out_data -0.8974460363388062 target [-0.89323644]\n",
            "jid 1058 out_data -0.774506688117981 target [-0.89323644]\n",
            "jid 1036 out_data -0.9283473491668701 target [-0.87199553]\n",
            "jid 1036 out_data -0.8092482686042786 target [-0.87199553]\n",
            "jid 1024 out_data -0.6498793959617615 target [-0.64784796]\n",
            "jid 1024 out_data -0.5342738032341003 target [-0.64784796]\n",
            "jid 1017 out_data -0.39301151037216187 target [-0.22526551]\n",
            "jid 1017 out_data -0.3016718924045563 target [-0.22526551]\n",
            "jid 1011 out_data -0.6781173348426819 target [-0.72833985]\n",
            "jid 1011 out_data -0.5634137392044067 target [-0.72833985]\n",
            "jid 998 out_data -0.7475531697273254 target [-0.68921185]\n",
            "jid 998 out_data -0.6500564813613892 target [-0.68921185]\n",
            "jid 977 out_data -0.6966608166694641 target [-0.56791504]\n",
            "jid 977 out_data -0.41460320353507996 target [-0.56791504]\n",
            "jid 976 out_data -0.7472721338272095 target [-0.65623253]\n",
            "jid 976 out_data -0.5599449276924133 target [-0.65623253]\n",
            "jid 964 out_data -0.8600533604621887 target [-0.78144215]\n",
            "jid 964 out_data -0.7265454530715942 target [-0.78144215]\n",
            "jid 963 out_data -0.9413303136825562 target [-0.85466741]\n",
            "jid 963 out_data -0.8426483869552612 target [-0.85466741]\n",
            "jid 950 out_data -0.6478490233421326 target [-0.51089994]\n",
            "jid 950 out_data -0.5515539646148682 target [-0.51089994]\n",
            "jid 938 out_data -0.7818563580513 target [-0.73169368]\n",
            "jid 938 out_data -0.6650386452674866 target [-0.73169368]\n",
            "jid 937 out_data -0.6839153170585632 target [-0.73057574]\n",
            "jid 937 out_data -0.6012924909591675 target [-0.73057574]\n",
            "jid 930 out_data -0.7640133500099182 target [-0.73281163]\n",
            "jid 930 out_data -0.6213230490684509 target [-0.73281163]\n",
            "jid 923 out_data -0.5770291090011597 target [-0.62269424]\n",
            "jid 923 out_data -0.45420995354652405 target [-0.62269424]\n",
            "jid 913 out_data -0.7436463236808777 target [-0.74846283]\n",
            "jid 913 out_data -0.6744641661643982 target [-0.74846283]\n",
            "jid 899 out_data -0.30407851934432983 target [0.13862493]\n",
            "jid 899 out_data -0.10950291901826859 target [0.13862493]\n",
            "jid 877 out_data -0.9104288816452026 target [-0.84572387]\n",
            "jid 877 out_data -0.7429036498069763 target [-0.84572387]\n",
            "jid 872 out_data -0.5519838333129883 target [-0.37506987]\n",
            "jid 872 out_data -0.37506338953971863 target [-0.37506987]\n",
            "jid 868 out_data -0.8617995977401733 target [-0.83119061]\n",
            "jid 868 out_data -0.783828616142273 target [-0.83119061]\n",
            "jid 856 out_data -0.3763304352760315 target [-0.26551146]\n",
            "jid 856 out_data -0.17624923586845398 target [-0.26551146]\n",
            "jid 844 out_data -0.9507243633270264 target [-0.86416993]\n",
            "jid 844 out_data -0.8163455128669739 target [-0.86416993]\n",
            "jid 829 out_data -0.20724906027317047 target [0.14086082]\n",
            "jid 829 out_data 0.15728548169136047 target [0.14086082]\n",
            "jid 823 out_data -0.511386513710022 target [0.97484628]\n",
            "jid 823 out_data -0.4630047380924225 target [0.97484628]\n",
            "jid 818 out_data -0.6064101457595825 target [-0.38848519]\n",
            "jid 818 out_data -0.34955963492393494 target [-0.38848519]\n",
            "jid 813 out_data -0.2820698618888855 target [0.80547792]\n",
            "jid 813 out_data 0.13905835151672363 target [0.80547792]\n",
            "jid 806 out_data -1.0098004341125488 target [-0.88988262]\n",
            "jid 806 out_data -0.8729615211486816 target [-0.88988262]\n",
            "jid 803 out_data -0.7354658842086792 target [-0.67859139]\n",
            "jid 803 out_data -0.7037439346313477 target [-0.67859139]\n",
            "jid 800 out_data -0.7406407594680786 target [-0.70206819]\n",
            "jid 800 out_data -0.695562481880188 target [-0.70206819]\n",
            "jid 793 out_data -0.9294649362564087 target [-0.90384469]\n",
            "jid 793 out_data -0.831077516078949 target [-0.90384469]\n",
            "jid 776 out_data -1.022053837776184 target [-0.96422582]\n",
            "jid 776 out_data -0.8624488711357117 target [-0.96422582]\n",
            "jid 772 out_data -0.44190770387649536 target [0.18613751]\n",
            "jid 772 out_data -0.3823479413986206 target [0.18613751]\n",
            "jid 757 out_data -1.0300180912017822 target [-0.96254891]\n",
            "jid 757 out_data -0.8946423530578613 target [-0.96254891]\n",
            "jid 756 out_data -0.9549708366394043 target [-0.89491336]\n",
            "jid 756 out_data -0.835412859916687 target [-0.89491336]\n",
            "jid 733 out_data -0.6565994024276733 target [-0.58971492]\n",
            "jid 733 out_data -0.5425570011138916 target [-0.58971492]\n",
            "jid 732 out_data -0.7721189260482788 target [-0.78591392]\n",
            "jid 732 out_data -0.7375409603118896 target [-0.78591392]\n",
            "jid 731 out_data -0.9188511371612549 target [-0.84628284]\n",
            "jid 731 out_data -0.7770283222198486 target [-0.84628284]\n",
            "jid 730 out_data -0.9099997282028198 target [-0.84851873]\n",
            "jid 730 out_data -0.7692972421646118 target [-0.84851873]\n",
            "jid 729 out_data -0.5746804475784302 target [-0.39295696]\n",
            "jid 729 out_data -0.4417772591114044 target [-0.39295696]\n",
            "jid 727 out_data -0.7073023915290833 target [-0.5941867]\n",
            "jid 727 out_data -0.6719568371772766 target [-0.5941867]\n",
            "jid 720 out_data -0.8239864110946655 target [-0.81777529]\n",
            "jid 720 out_data -0.7285854816436768 target [-0.81777529]\n",
            "jid 698 out_data -0.9372375011444092 target [-0.90217999]\n",
            "jid 698 out_data -0.82448410987854 target [-0.90217999]\n",
            "jid 693 out_data -0.7221160531044006 target [-0.68026831]\n",
            "jid 693 out_data -0.580685019493103 target [-0.68026831]\n",
            "jid 689 out_data -0.909935474395752 target [-0.77585243]\n",
            "jid 689 out_data -0.7444823384284973 target [-0.77585243]\n",
            "jid 679 out_data -0.6200318336486816 target [-0.61708731]\n",
            "jid 679 out_data -0.5757770538330078 target [-0.61708731]\n",
            "jid 668 out_data -1.0116502046585083 target [-0.93627725]\n",
            "jid 668 out_data -0.8860088586807251 target [-0.93627725]\n",
            "jid 663 out_data -0.8254871368408203 target [-0.75013974]\n",
            "jid 663 out_data -0.7151667475700378 target [-0.75013974]\n",
            "jid 658 out_data -0.6638624668121338 target [-0.58982179]\n",
            "jid 658 out_data -0.5731568336486816 target [-0.58982179]\n",
            "jid 657 out_data -0.8890271186828613 target [-0.83286752]\n",
            "jid 657 out_data -0.7834109663963318 target [-0.83286752]\n",
            "jid 651 out_data -0.701607882976532 target [-0.69368362]\n",
            "jid 651 out_data -0.5649058818817139 target [-0.69368362]\n",
            "jid 639 out_data -0.43905389308929443 target [-0.3432085]\n",
            "jid 639 out_data -0.25109079480171204 target [-0.3432085]\n",
            "jid 637 out_data -0.8301621079444885 target [-0.82001118]\n",
            "jid 637 out_data -0.7457119226455688 target [-0.82001118]\n",
            "jid 623 out_data -0.8683027029037476 target [-0.76690889]\n",
            "jid 623 out_data -0.7083240151405334 target [-0.76690889]\n",
            "jid 617 out_data -0.5771321058273315 target [-0.61989939]\n",
            "jid 617 out_data -0.4503588378429413 target [-0.61989939]\n",
            "jid 612 out_data -0.48682695627212524 target [-0.40245947]\n",
            "jid 612 out_data -0.3577590584754944 target [-0.40245947]\n",
            "jid 610 out_data -0.6854836940765381 target [-0.67300168]\n",
            "jid 610 out_data -0.5247189998626709 target [-0.67300168]\n",
            "jid 591 out_data -0.7150338888168335 target [-0.62213527]\n",
            "jid 591 out_data -0.6492557525634766 target [-0.62213527]\n",
            "jid 578 out_data -0.9877372980117798 target [-0.91894913]\n",
            "jid 578 out_data -0.8520983457565308 target [-0.91894913]\n",
            "jid 559 out_data -0.7773423790931702 target [-0.7699787]\n",
            "jid 559 out_data -0.6872271299362183 target [-0.7699787]\n",
            "jid 539 out_data -0.7588528990745544 target [-0.70653997]\n",
            "jid 539 out_data -0.6832229495048523 target [-0.70653997]\n",
            "jid 536 out_data -0.6117488741874695 target [-0.59583125]\n",
            "jid 536 out_data -0.5259482860565186 target [-0.59583125]\n",
            "jid 518 out_data -0.7839875221252441 target [-0.75125769]\n",
            "jid 518 out_data -0.6619220972061157 target [-0.75125769]\n",
            "jid 495 out_data -0.9572921991348267 target [-0.90929683]\n",
            "jid 495 out_data -0.8726505041122437 target [-0.90929683]\n",
            "jid 480 out_data -0.5639384984970093 target [-0.43487982]\n",
            "jid 480 out_data -0.4584764540195465 target [-0.43487982]\n",
            "jid 456 out_data -0.9519636631011963 target [-0.8529905]\n",
            "jid 456 out_data -0.7294174432754517 target [-0.8529905]\n",
            "jid 453 out_data -0.9305545091629028 target [-0.84795975]\n",
            "jid 453 out_data -0.8043302297592163 target [-0.84795975]\n",
            "jid 450 out_data -0.8140867948532104 target [-0.79653438]\n",
            "jid 450 out_data -0.7267091274261475 target [-0.79653438]\n",
            "jid 449 out_data -1.003980278968811 target [-0.92621576]\n",
            "jid 449 out_data -0.885991096496582 target [-0.92621576]\n",
            "jid 442 out_data -0.14767225086688995 target [0.93515931]\n",
            "jid 442 out_data 0.21492725610733032 target [0.93515931]\n",
            "jid 428 out_data -0.8890578746795654 target [-0.8569033]\n",
            "jid 428 out_data -0.8139685392379761 target [-0.8569033]\n",
            "jid 413 out_data -0.7851431369781494 target [-0.82448295]\n",
            "jid 413 out_data -0.7312927842140198 target [-0.82448295]\n",
            "jid 405 out_data -1.0096684694290161 target [-0.90329793]\n",
            "jid 405 out_data -0.8441928625106812 target [-0.90329793]\n",
            "jid 395 out_data -0.9681421518325806 target [-0.90217999]\n",
            "jid 395 out_data -0.8297626376152039 target [-0.90217999]\n",
            "jid 373 out_data -0.1786862164735794 target [0.83174958]\n",
            "jid 373 out_data 0.10267224162817001 target [0.83174958]\n",
            "jid 371 out_data -0.5660246014595032 target [-0.53158189]\n",
            "jid 371 out_data -0.5352065563201904 target [-0.53158189]\n",
            "jid 365 out_data -0.5162992477416992 target [1.00344212]\n",
            "jid 365 out_data -0.2594035863876343 target [1.00344212]\n",
            "jid 363 out_data -0.2891787886619568 target [-0.31470095]\n",
            "jid 363 out_data -0.1352212131023407 target [-0.31470095]\n",
            "jid 357 out_data -0.587231457233429 target [-0.4745668]\n",
            "jid 357 out_data -0.5429693460464478 target [-0.4745668]\n",
            "jid 318 out_data -0.6555026769638062 target [-0.66776089]\n",
            "jid 318 out_data -0.5713742971420288 target [-0.66776089]\n",
            "jid 310 out_data -0.8377578258514404 target [-0.7568474]\n",
            "jid 310 out_data -0.7476915121078491 target [-0.7568474]\n",
            "jid 304 out_data -0.930014967918396 target [-0.88596982]\n",
            "jid 304 out_data -0.8427314162254333 target [-0.88596982]\n",
            "jid 281 out_data -1.0416762828826904 target [-0.94745668]\n",
            "jid 281 out_data -0.8749138116836548 target [-0.94745668]\n",
            "jid 277 out_data -1.0267269611358643 target [-0.98490777]\n",
            "jid 277 out_data -0.8660451769828796 target [-0.98490777]\n",
            "jid 245 out_data -0.9734344482421875 target [-0.85131358]\n",
            "jid 245 out_data -0.8498455882072449 target [-0.85131358]\n",
            "jid 244 out_data -0.9202584028244019 target [-0.80771381]\n",
            "jid 244 out_data -0.7731634378433228 target [-0.80771381]\n",
            "jid 239 out_data -0.6351695656776428 target [-0.58588874]\n",
            "jid 239 out_data -0.5457667708396912 target [-0.58588874]\n",
            "jid 236 out_data -0.9014655351638794 target [-0.84795975]\n",
            "jid 236 out_data -0.771145224571228 target [-0.84795975]\n",
            "jid 222 out_data -1.0100455284118652 target [-0.93124651]\n",
            "jid 222 out_data -0.8522586822509766 target [-0.93124651]\n",
            "jid 199 out_data -0.05247066915035248 target [-0.34935718]\n",
            "jid 199 out_data -0.05975765734910965 target [-0.34935718]\n",
            "jid 189 out_data -0.5952253341674805 target [-0.51201789]\n",
            "jid 189 out_data -0.5819125175476074 target [-0.51201789]\n",
            "jid 186 out_data -0.8978948593139648 target [-0.90665176]\n",
            "jid 186 out_data -0.8200492262840271 target [-0.90665176]\n",
            "jid 167 out_data -0.833919882774353 target [-0.75237563]\n",
            "jid 167 out_data -0.6845014691352844 target [-0.75237563]\n",
            "jid 161 out_data -0.5375153422355652 target [-0.49916154]\n",
            "jid 161 out_data -0.4701126515865326 target [-0.49916154]\n",
            "jid 149 out_data -0.7555093765258789 target [-0.72107323]\n",
            "jid 149 out_data -0.6777179837226868 target [-0.72107323]\n",
            "jid 144 out_data -0.7354167699813843 target [-0.68474008]\n",
            "jid 144 out_data -0.6849514245986938 target [-0.68474008]\n",
            "jid 129 out_data -0.6113599538803101 target [-0.63666853]\n",
            "jid 129 out_data -0.532194972038269 target [-0.63666853]\n",
            "jid 127 out_data -0.9485360383987427 target [-0.98667387]\n",
            "jid 127 out_data -0.7993741631507874 target [-0.98667387]\n",
            "jid 117 out_data -0.6098132133483887 target [-0.59934486]\n",
            "jid 117 out_data -0.5514029860496521 target [-0.59934486]\n",
            "jid 101 out_data -1.0263696908950806 target [-0.92845165]\n",
            "jid 101 out_data -0.806916356086731 target [-0.92845165]\n",
            "jid 96 out_data -1.0172685384750366 target [-0.90609279]\n",
            "jid 96 out_data -0.7928022146224976 target [-0.90609279]\n",
            "jid 89 out_data -0.8540920615196228 target [-0.70933482]\n",
            "jid 89 out_data -0.6394649744033813 target [-0.70933482]\n",
            "jid 78 out_data -0.5876927375793457 target [-0.56725431]\n",
            "jid 78 out_data -0.5360183119773865 target [-0.56725431]\n",
            "jid 67 out_data -0.979198694229126 target [-0.93795416]\n",
            "jid 67 out_data -0.8475433588027954 target [-0.93795416]\n",
            "jid 45 out_data -0.8895881175994873 target [-0.79318055]\n",
            "jid 45 out_data -0.7617307901382446 target [-0.79318055]\n",
            "jid 30 out_data -0.9188551902770996 target [-0.84572387]\n",
            "jid 30 out_data -0.7770161628723145 target [-0.84572387]\n",
            "jid 20 out_data -0.694125235080719 target [-0.82839575]\n",
            "jid 20 out_data -0.6498217582702637 target [-0.82839575]\n",
            "jid 6 out_data -0.9153989553451538 target [-0.86696478]\n",
            "jid 6 out_data -0.7646123170852661 target [-0.86696478]\n",
            "jid 2 out_data -0.6175286769866943 target [-0.39686976]\n",
            "jid 2 out_data -0.47609904408454895 target [-0.39686976]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from alignn.models.alignn_atomwise import ALIGNNAtomWise, ALIGNNAtomWiseConfig\n",
        "import torch\n",
        "from jarvis.core.atoms import Atoms\n",
        "from alignn.graphs import Graph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "filename_scratch = 'phonons/current_model.pt'\n",
        "filename_ft = 'fine-tune/current_model.pt'\n",
        "device = \"cpu\"\n",
        "\n",
        "# Initialize two separate models\n",
        "model_scratch = ALIGNNAtomWise(ALIGNNAtomWiseConfig(name ='alignn_atomwise',gcn_layers = 4, atom_input_features = 92, alignn_layers = 4, output_features = 1 ))\n",
        "model_ft = ALIGNNAtomWise(ALIGNNAtomWiseConfig(name ='alignn_atomwise',gcn_layers = 4, atom_input_features = 92, alignn_layers = 4, output_features = 1 ))\n",
        "\n",
        "# Load state dicts seperately\n",
        "state_dict_scratch = torch.load(filename_scratch, map_location=device)\n",
        "state_dict_ft = torch.load(filename_ft, map_location=device)\n",
        "\n",
        "# Load the model parameters\n",
        "model_scratch.load_state_dict(state_dict_scratch)\n",
        "model_ft.load_state_dict(state_dict_ft)\n",
        "\n",
        "model_scratch.eval()\n",
        "model_ft.eval()\n",
        "cutoff = 8.0\n",
        "model_scratch = model_scratch.to(device)\n",
        "model_ft = model_ft.to(device)\n",
        "max_neighbors = 12\n",
        "\n",
        "# Save the current standard output (e.g., the console) to a variable\n",
        "original_stdout = sys.stdout\n",
        "results_scratch = []\n",
        "results_ft = []\n",
        "for i in test:\n",
        "    atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "    g, lg = Graph.atom_dgl_multigraph(\n",
        "        atoms, cutoff=float(cutoff), max_neighbors=max_neighbors)\n",
        "    out_data_scratch = (model_scratch([g.to(device), lg.to(device),[atoms.lattice.a,atoms.lattice.b,atoms.lattice.c]])['out']\n",
        "        .detach()\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "        .flatten()\n",
        "        .tolist())\n",
        "    results_scratch.append({ \"jid\": i['jid'],\n",
        "            \"out_data\": out_data_scratch[0],\n",
        "            \"target\": i['target']\n",
        "        })\n",
        "    print(\"jid\", i['jid'],\n",
        "            \"out_data\", out_data_scratch[0],\n",
        "            \"target\", i['target'])\n",
        "    out_data_ft = (model_ft([g.to(device), lg.to(device),[atoms.lattice.a,atoms.lattice.b,atoms.lattice.c]])['out']\n",
        "        .detach()\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "        .flatten()\n",
        "        .tolist())\n",
        "    results_ft.append({ \"jid\": i['jid'],\n",
        "            \"out_data\": out_data_ft[0],\n",
        "            \"target\": i['target']\n",
        "        })\n",
        "    print(\"jid\", i['jid'],\n",
        "            \"out_data\", out_data_ft[0],\n",
        "            \"target\", i['target'])\n",
        "csv_scratch = pd.DataFrame(results_scratch)\n",
        "csv_ft = pd.DataFrame(results_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Undo the standardization and normalization operation"
      ],
      "metadata": {
        "id": "Xcwtr66E_6-k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNJ8NkJPYSNt"
      },
      "source": [
        "Remember to undo the normalization and standardization operations in the same order as mentioned before analyzing the results. The follwing formulas are used in the same order to undo the operations.\n",
        "\n",
        "**Unnormalized_value =  $\\frac{(\\textbf{Normalized value} + 1)}{2}$  $\\times$ (Original_scaled_max - Original_scaled_min) + Original_scaled_min**\n",
        "\n",
        "**Original_value = (Unnormalized_value $\\times$ Original_std_dev) + Original_mean**\n",
        "\n",
        "```\n",
        "      Original_std_dev => standard deviation of unscaled and unnormalized training dataset\n",
        "      Original_mean => mean of unscaled and unnormalized training dataset\n",
        "      Original_scaled_max =>  maxima of the scaled training dataset\n",
        "      Original_scaled_min =>  minima of the scaled training dataset\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOtXdn2sX7ja",
        "outputId": "ef638a75-df33-4820-efa5-454c321b9d41"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/phonons/TRAIN-UNSTD.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/phonons/TRAIN-UNSTD.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(train_data)\n",
            "File \u001b[0;32m~/anaconda3/envs/pymatgen/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/phonons/TRAIN-UNSTD.json'"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import statistics as s\n",
        "scaler = StandardScaler()\n",
        "with open('/content/phonons/TRAIN-UNSTD.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "df_train = pd.DataFrame.from_dict(train_data)\n",
        "\n",
        "#Get min and max from this array\n",
        "arr1 = scaler.fit_transform(df_train[['target']]).flatten()\n",
        "#Get range from standardized data\n",
        "original_min = min(arr1)\n",
        "original_max = max(arr1)\n",
        "\n",
        "\n",
        "#Get the mean and std_dev from this array\n",
        "raw_target = np.array(df_train['target'])\n",
        "# Standardized data mean, stdev and range\n",
        "original_mean = s.mean(raw_target)\n",
        "original_std = s.stdev(raw_target)\n",
        "\n",
        "\n",
        "def undo_normalization_and_standardization(normalized_array, original_mean, original_std, original_min, original_max):\n",
        "    # Step 1: Undo the normalization\n",
        "    unnormalized_array = (normalized_array + 1) / 2 * (original_max - original_min) + original_min\n",
        "\n",
        "    # Step 2: Undo the standardization\n",
        "    original_array = unnormalized_array * original_std + original_mean\n",
        "\n",
        "    return original_array"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze the performance metrics"
      ],
      "metadata": {
        "id": "IdRlk-HEAFqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1oxg4vSZQme",
        "outputId": "9423ed72-749f-4bea-aafa-2e47ee9bc55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scratch model R2 score 0.49318306976955495\n",
            "Scratch model MAE score 231.0842744984917\n",
            "############################################\n",
            "Fine-tuned model R2 score 0.6514972649227397\n",
            "Fine-tuned model MAE score 212.85688423034946\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "###################Print results of the scratch model##################\n",
        "pred = undo_normalization_and_standardization(np.array(csv_scratch[\"out_data\"]), original_mean, original_std, original_min, original_max)\n",
        "target = undo_normalization_and_standardization(np.array(csv_scratch[\"target\"]), original_mean, original_std, original_min, original_max)\n",
        "r2= r2_score(target, pred)\n",
        "mae = mean_absolute_error(target, pred)\n",
        "print('Scratch model R2 score',r2)\n",
        "print('Scratch model MAE score',mae)\n",
        "print('############################################')\n",
        "##################Print results of the fine-tuned model################\n",
        "pred = undo_normalization_and_standardization(np.array(csv_ft[\"out_data\"]), original_mean, original_std, original_min, original_max)\n",
        "target = undo_normalization_and_standardization(np.array(csv_ft[\"target\"]), original_mean, original_std, original_min, original_max)\n",
        "r2= r2_score(target, pred)\n",
        "mae = mean_absolute_error(target, pred)\n",
        "print('Fine-tuned model R2 score',r2)\n",
        "print('Fine-tuned model MAE score',mae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using other pre-trained models from ALIGNN\n",
        "\n",
        "We'll now utilize another pre-trained model from the repository. The following code utilizes a model trained on JARVIS-DFT formation energy.\n",
        "\n",
        "Alter the snippet of code below to compare the formation energies of the structures generated from crystalLLM and Chemeleon\n",
        "Refer to the [documentation](https://github.com/usnistgov/alignn) for other pre-trained models"
      ],
      "metadata": {
        "id": "VK1pFnuQAKzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDxIcBo-aeQ2",
        "outputId": "0885c8a4-dfa0-4e87-8f61-4167f0f31d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "list index out of range\n"
          ]
        }
      ],
      "source": [
        "atoms = Atoms.from_cif('/content/Si.cif')# Load the pickle/cif files that you generated in the previous exercise\n",
        "atoms.write_poscar('POSCAR_trial')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtDp4KCCden6",
        "outputId": "73c85b63-1605-4fff-a09a-ad1eeb9d1d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n",
            "100% 47.5M/47.5M [00:03<00:00, 14.4MiB/s]\n",
            "Using chk file jv_formation_energy_peratom_alignn/checkpoint_300.pt from  ['jv_formation_energy_peratom_alignn/checkpoint_300.pt']\n",
            "Path /usr/local/bin/jv_formation_energy_peratom_alignn.zip\n",
            "Config /content/jv_formation_energy_peratom_alignn/config.json\n",
            "Predicted value: jv_formation_energy_peratom_alignn POSCAR_trial [0.0037604793906211853]\n"
          ]
        }
      ],
      "source": [
        "!pretrained.py --model_name jv_formation_energy_peratom_alignn  --file_format poscar --file_path POSCAR_trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_tixst6dfIi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}